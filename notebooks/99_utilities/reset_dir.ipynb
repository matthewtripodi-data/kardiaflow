{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# reset_dir.ipynb\n",
    "# Full clean slate for KardiaFlow:\n",
    "# 1. Drop the databases and Delta tables from the metastore\n",
    "# 2. Delete all underlying storage for bronze/silver/raw/source/enriched including checkpoints\n",
    "# 3. Clear any in-memory Spark catalog cache to avoid stale metadata\n",
    "# This leaves nothing persistent that would affect a fresh bootstrap/run\n",
    "\n",
    "from kflow.config import bronze_paths, silver_paths\n",
    "from kflow.auth_adls import ensure_adls_oauth\n",
    "\n",
    "spark.sql(\"USE CATALOG hive_metastore\")\n",
    "\n",
    "# Configure ABFS OAuth once for this session\n",
    "ensure_adls_oauth(validate_path=\"\")\n",
    "\n",
    "def safe_rm(path: str, description: str) -> None:\n",
    "    \"\"\"\n",
    "    Attempt to remove a path via dbutils.fs.rm recursively.\n",
    "    On failure, log and continue.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dbutils.fs.rm(path, recurse=True)\n",
    "        print(f\"Removed {description}: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  (ignore) failed removing {description} {path}: {e}\")\n",
    "\n",
    "\n",
    "def safe_drop_table(full_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Drop a table, preferring PURGE. If that fails, fall back to a normal drop.\n",
    "    Clears cache before attempting to minimize stale-metadata errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spark.catalog.clearCache()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"Dropping table {full_name}\")\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {full_name} PURGE\")\n",
    "        print(f\"Dropped table {full_name} with PURGE\")\n",
    "    except Exception as e:\n",
    "        print(f\"  (ignore) PURGE failed for {full_name}: {e}\")\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {full_name}\")\n",
    "            print(f\"Dropped table {full_name} without PURGE fallback\")\n",
    "        except Exception as e2:\n",
    "            print(f\"  (ignore) failed dropping {full_name}: {e2}\")\n",
    "\n",
    "\n",
    "# 1. Drop the Delta tables from the metastore with PURGE so no leftover catalog entries remain.\n",
    "to_drop = [\n",
    "    \"kardia_bronze.bronze_encounters\",\n",
    "    \"kardia_bronze.bronze_claims\",\n",
    "    \"kardia_bronze.bronze_patients\",\n",
    "    \"kardia_bronze.bronze_providers\",\n",
    "    \"kardia_bronze.bronze_feedback\",\n",
    "    \"kardia_silver.silver_encounters\",\n",
    "    \"kardia_silver.silver_claims\",\n",
    "    \"kardia_silver.silver_patients\",\n",
    "    \"kardia_silver.silver_providers\",\n",
    "    \"kardia_silver.silver_feedback\",\n",
    "    \"kardia_silver.silver_encounters_enriched\"\n",
    "]\n",
    "\n",
    "for full_name in to_drop:\n",
    "    safe_drop_table(full_name)\n",
    "\n",
    "\n",
    "# 2. Remove all underlying storage and checkpoint directories for each dataset.\n",
    "datasets = (\"encounters\", \"claims\", \"patients\", \"providers\", \"feedback\")\n",
    "for name in datasets:\n",
    "    # Bronze layer paths\n",
    "    P = bronze_paths(name)\n",
    "    print(f\"\\nCleaning bronze layer for '{name}':\")\n",
    "    safe_rm(P.bronze, f\"bronze data for {name}\")\n",
    "    safe_rm(P.checkpoint, f\"bronze checkpoint for {name}\")\n",
    "\n",
    "    # Raw / source input (unstructured source zone)\n",
    "    print(f\"Cleaning raw/source layer for '{name}':\")\n",
    "    safe_rm(P.raw, f\"raw/source data for {name}\")\n",
    "\n",
    "    # Silver layer paths\n",
    "    S = silver_paths(name)\n",
    "    print(f\"Cleaning silver layer for '{name}':\")\n",
    "    safe_rm(S.path, f\"silver data for {name}\")\n",
    "    safe_rm(S.checkpoint, f\"silver checkpoint for {name}\")\n",
    "\n",
    "\n",
    "# 3. Remove enriched target (separate from the core silver datasets)\n",
    "print(\"\\nCleaning enriched target 'encounters_enriched':\")\n",
    "S_enriched = silver_paths(\"encounters_enriched\")\n",
    "safe_rm(S_enriched.path, \"enriched path\")\n",
    "safe_rm(S_enriched.checkpoint, \"enriched checkpoint\")\n",
    "\n",
    "\n",
    "# 4. Drop the databases to eliminate residual namespace/catalog artifacts\n",
    "print(\"\\nDropping databases (if they exist):\")\n",
    "try:\n",
    "    spark.sql(\"DROP DATABASE IF EXISTS kardia_bronze CASCADE\")\n",
    "    print(\"Dropped database kardia_bronze\")\n",
    "except Exception as e:\n",
    "    print(f\"  (ignore) failed dropping database kardia_bronze: {e}\")\n",
    "\n",
    "try:\n",
    "    spark.sql(\"DROP DATABASE IF EXISTS kardia_silver CASCADE\")\n",
    "    print(\"Dropped database kardia_silver\")\n",
    "except Exception as e:\n",
    "    print(f\"  (ignore) failed dropping database kardia_silver: {e}\")\n",
    "\n",
    "try:\n",
    "    spark.sql(\"DROP DATABASE IF EXISTS kardia_gold CASCADE\")\n",
    "    print(\"Dropped database kardia_gold\")\n",
    "except Exception as e:\n",
    "    print(f\"  (ignore) failed dropping database kardia_gold: {e}\")\n",
    "\n",
    "\n",
    "# 5. Clear in-memory catalog/cache to avoid stale metadata in the current session\n",
    "print(\"\\nClearing Spark catalog cache.\")\n",
    "try:\n",
    "    spark.catalog.clearCache()\n",
    "except Exception as e:\n",
    "    print(f\"  (ignore) failed clearing cache: {e}\")\n",
    "\n",
    "print(\"\\nWipe complete.\")"
   ],
   "id": "4f5fdaf6a4f2956a"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
