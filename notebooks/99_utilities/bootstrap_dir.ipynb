{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# bootstrap_dir.ipynb\n",
    "# Creates the five raw-input folders in ADLS\n",
    "# and drops in the first sample file for each dataset.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# 1  Fixed ADLS path prefix\n",
    "lake_root = \"abfss://lake@kardiaadlsdemo.dfs.core.windows.net\"\n",
    "\n",
    "# 2  Create the raw/source folders (idempotent)\n",
    "for ds in (\"encounters\", \"claims\", \"patients\", \"providers\", \"feedback\"):\n",
    "    raw_dir = f\"{lake_root}/{ds}\"\n",
    "    dbutils.fs.mkdirs(raw_dir)\n",
    "    print(\"Ensured\", raw_dir)\n",
    "\n",
    "# 3  Copy one local sample into each raw folder\n",
    "repo_root = Path().cwd()\n",
    "samples = {\n",
    "    \"encounters\": (\"data/encounters/encounters_part_1.avro\",  \"avro\"),\n",
    "    \"claims\":     (\"data/claims/claims_part_1.parquet\",       \"parquet\"),\n",
    "    \"feedback\":   (\"data/feedback/feedback_part_1.jsonl\",     \"json\"),\n",
    "    \"patients\":   (\"data/patients/patients_part_1.csv\",       \"csv\"),\n",
    "    \"providers\":  (\"data/providers/providers_part_1.tsv\",     \"tsv\"),\n",
    "}\n",
    "\n",
    "for ds, (rel_path, fmt) in samples.items():\n",
    "    src_file   = repo_root / rel_path\n",
    "    target_dir = f\"{lake_root}/{ds}\"\n",
    "\n",
    "    if not src_file.exists():\n",
    "        print(f\"SKIP {ds}: {src_file} not found\")\n",
    "        continue\n",
    "\n",
    "    print(f\"→ {ds}: loading {src_file.name} and writing to {target_dir}\")\n",
    "\n",
    "    if fmt == \"parquet\":\n",
    "        (spark.read.parquet(str(src_file))\n",
    "              .write.mode(\"overwrite\").parquet(target_dir))\n",
    "\n",
    "    elif fmt == \"avro\":\n",
    "        (spark.read.format(\"avro\").load(str(src_file))\n",
    "              .write.mode(\"overwrite\").format(\"avro\").save(target_dir))\n",
    "\n",
    "    elif fmt == \"json\":\n",
    "        (spark.read.option(\"multiLine\", False).json(str(src_file))\n",
    "              .write.mode(\"overwrite\").json(target_dir))\n",
    "\n",
    "    elif fmt == \"csv\":\n",
    "        (spark.read.option(\"header\", True).csv(str(src_file))\n",
    "              .write.mode(\"overwrite\").option(\"header\", True).csv(target_dir))\n",
    "\n",
    "    elif fmt == \"tsv\":\n",
    "        (spark.read.option(\"header\", True).option(\"sep\", \"\\t\").csv(str(src_file))\n",
    "              .write.mode(\"overwrite\").option(\"header\", True).option(\"sep\", \"\\t\").csv(target_dir))\n",
    "\n",
    "print(\"\\nBootstrap complete – raw folders ready for Auto Loader.\")"
   ],
   "id": "7752474d7e6e8a98"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
