{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# bootstrap_dir.ipynb\n",
    "# Goal: Ensure medallion + raw folders exist in ADLS and seed one sample file per dataset.\n",
    "\n",
    "# Install kflow wheel\n",
    "%pip install -q --no-deps --no-index --find-links=/dbfs/Shared/libs kflow\n",
    "dbutils.library.restartPython()\n",
    "\n",
    "from pathlib import Path\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from kflow.auth_adls import ensure_adls_oauth\n",
    "from kflow.config import LAKE_ROOT  # uses abfss://.../kardia\n",
    "\n",
    "# 1) Configure OAuth for this session and validate access\n",
    "abfss_base = ensure_adls_oauth(validate_path=\"\")   # returns \"abfss://lake@...dfs.core.windows.net\"\n",
    "lake_root  = LAKE_ROOT\n",
    "print(f\"ABFSS base       : {abfss_base}\")\n",
    "print(f\"Lake root        : {lake_root}\")\n",
    "\n",
    "# 2) Create medallion layer folders (idempotent)\n",
    "for layer in (\"bronze\", \"silver\", \"gold\", \"_schemas\", \"_checkpoints\", \"_quarantine\"):\n",
    "    p = f\"{lake_root}/{layer}\"\n",
    "    dbutils.fs.mkdirs(p)\n",
    "    print(\"Ensured\", p)\n",
    "\n",
    "# 3) Create raw/source folders (idempotent)\n",
    "for ds in (\"encounters\", \"claims\", \"patients\", \"providers\", \"feedback\"):\n",
    "    raw_dir = f\"{abfss_base}/{ds}\"\n",
    "    dbutils.fs.mkdirs(raw_dir)\n",
    "    print(\"Ensured\", raw_dir)\n",
    "\n",
    "# 4) Copy one local sample into each raw folder\n",
    "#    Assumes the notebook lives in a Repo with the /data/<ds>/... files committed.\n",
    "#    Path().cwd() resolves inside the job run to the repo working directory.\n",
    "repo_root = Path().cwd()\n",
    "samples = {\n",
    "    \"encounters\": (\"data/encounters/encounters_part_1.avro\", \"avro\"),\n",
    "    \"claims\"    : (\"data/claims/claims_part_1.parquet\", \"parquet\"),\n",
    "    \"feedback\"  : (\"data/feedback/feedback_part_1.jsonl\", \"json\"),\n",
    "    \"patients\"  : (\"data/patients/patients_part_1.csv\", \"csv\"),\n",
    "    \"providers\" : (\"data/providers/providers_part_1.tsv\", \"tsv\"),\n",
    "}\n",
    "\n",
    "for ds, (rel_path, fmt) in samples.items():\n",
    "    src_file = repo_root / rel_path\n",
    "    target_dir = f\"{abfss_base}/{ds}\"\n",
    "\n",
    "    if not src_file.exists():\n",
    "        print(f\"SKIP {ds}: {src_file} not found (OK if you haven’t added samples locally).\")\n",
    "        continue\n",
    "\n",
    "    print(f\"→ {ds}: loading {src_file.name} and writing to {target_dir}\")\n",
    "\n",
    "    if fmt == \"parquet\":\n",
    "        (spark.read.parquet(str(src_file))\n",
    "         .write.mode(\"overwrite\").parquet(target_dir))\n",
    "\n",
    "    elif fmt == \"avro\":\n",
    "        (spark.read.format(\"avro\").load(str(src_file))\n",
    "         .write.mode(\"overwrite\").format(\"avro\").save(target_dir))\n",
    "\n",
    "    elif fmt == \"json\":\n",
    "        (spark.read.option(\"multiLine\", False).json(str(src_file))\n",
    "         .write.mode(\"overwrite\").json(target_dir))\n",
    "\n",
    "    elif fmt == \"csv\":\n",
    "        (spark.read.option(\"header\", True).csv(str(src_file))\n",
    "         .write.mode(\"overwrite\").option(\"header\", True).csv(target_dir))\n",
    "\n",
    "    elif fmt == \"tsv\":\n",
    "        (spark.read.option(\"header\", True).option(\"sep\", \"\\t\").csv(str(src_file))\n",
    "         .write.mode(\"overwrite\").option(\"header\", True).option(\"sep\", \"\\t\").csv(target_dir))\n",
    "\n",
    "print(\"\\nBootstrap complete – medallion & raw folders ready.\")"
   ],
   "id": "7752474d7e6e8a98"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
