{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# bootstrap_dir.ipynb\n",
    "# Purpose:\n",
    "# - Create medallion folders (bronze/silver/gold/etc.) in ADLS Gen2\n",
    "# - Create raw input folders for each dataset\n",
    "# - Load one sample file per dataset to its corresponding raw folder\n",
    "\n",
    "# Install kflow and restart Python kernel\n",
    "%pip install -q --no-deps --no-index --find-links=/dbfs/Shared/libs kflow\n",
    "dbutils.library.restartPython()\n",
    "\n",
    "from pathlib import Path\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from kflow.auth_adls import ensure_adls_oauth\n",
    "from kflow.config import LAKE_ROOT  # uses abfss://.../kardia\n",
    "\n",
    "# 1) Configure ADLS OAuth and resolve paths\n",
    "abfss_base = ensure_adls_oauth(validate_path=\"\")\n",
    "lake_root = LAKE_ROOT\n",
    "print(f\"ABFSS base: {abfss_base}\")\n",
    "print(f\"Lake root: {lake_root}\")\n",
    "\n",
    "# 2) Create medallion layer directories\n",
    "for layer in (\"bronze\", \"silver\", \"gold\", \"_schemas\", \"_checkpoints\", \"_quarantine\"):\n",
    "    p = f\"{lake_root}/{layer}\"\n",
    "    dbutils.fs.mkdirs(p)\n",
    "    print(\"Ensured\", p)\n",
    "\n",
    "# 3) Create raw input directories for each dataset\n",
    "for ds in (\"encounters\", \"claims\", \"patients\", \"providers\", \"feedback\"):\n",
    "    raw_dir = f\"{abfss_base}/{ds}\"\n",
    "    dbutils.fs.mkdirs(raw_dir)\n",
    "    print(\"Ensured\", raw_dir)\n",
    "\n",
    "# 4) Upload one local sample file per dataset to its raw folder\n",
    "repo_root = Path().cwd()\n",
    "samples = {\n",
    "    \"encounters\": (\"data/encounters/encounters_part_1.avro\", \"avro\"),\n",
    "    \"claims\":     (\"data/claims/claims_part_1.parquet\", \"parquet\"),\n",
    "    \"feedback\":   (\"data/feedback/feedback_part_1.jsonl\", \"json\"),\n",
    "    \"patients\":   (\"data/patients/patients_part_1.csv\", \"csv\"),\n",
    "    \"providers\":  (\"data/providers/providers_part_1.tsv\", \"tsv\"),\n",
    "}\n",
    "\n",
    "for ds, (rel_path, fmt) in samples.items():\n",
    "    src_file = repo_root / rel_path\n",
    "    target_dir = f\"{abfss_base}/{ds}\"\n",
    "\n",
    "    if not src_file.exists():\n",
    "        print(f\"SKIP {ds}: {src_file} not found.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"→ {ds}: loading {src_file.name} and writing to {target_dir}\")\n",
    "\n",
    "    if fmt == \"parquet\":\n",
    "        (spark.read.parquet(str(src_file))\n",
    "         .write.mode(\"overwrite\").parquet(target_dir))\n",
    "\n",
    "    elif fmt == \"avro\":\n",
    "        (spark.read.format(\"avro\").load(str(src_file))\n",
    "         .write.mode(\"overwrite\").format(\"avro\").save(target_dir))\n",
    "\n",
    "    elif fmt == \"json\":\n",
    "        (spark.read.option(\"multiLine\", False).json(str(src_file))\n",
    "         .write.mode(\"overwrite\").json(target_dir))\n",
    "\n",
    "    elif fmt == \"csv\":\n",
    "        (spark.read.option(\"header\", True).csv(str(src_file))\n",
    "         .write.mode(\"overwrite\").option(\"header\", True).csv(target_dir))\n",
    "\n",
    "    elif fmt == \"tsv\":\n",
    "        (spark.read.option(\"header\", True).option(\"sep\", \"\\t\").csv(str(src_file))\n",
    "         .write.mode(\"overwrite\").option(\"header\", True).option(\"sep\", \"\\t\").csv(target_dir))\n",
    "\n",
    "print(\"\\nBootstrap complete – medallion and raw folders ready.\")"
   ],
   "id": "7752474d7e6e8a98"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
