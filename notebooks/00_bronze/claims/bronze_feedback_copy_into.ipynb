{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# bronze_feedback_copy_into.ipynb\n",
    "# SOURCE:  JSONâ€‘lines feedback files in ADLS at abfss://raw@kardiaadlsdemo.dfs.core.windows.net/feedback/  \n",
    "# OUTPUT: `kardia_bronze.bronze_feedback` with Change Data Feed enabled  \n",
    "# TRIGGER: Incremental batch; append to Delta table with schema evolution enabled  \n",
    "\n",
    "# NOTE: Using Auto Loader for JSONL; schema evolves automatically via schemaLocation.\n",
    "\n",
    "from kflow.config import bronze_paths, raw_path, current_batch_id, ensure_adls_auth\n",
    "from kflow.display_utils import show_history\n",
    "\n",
    "from pyspark.sql.types import (StructType, StructField, StringType, IntegerType,\n",
    "                               ArrayType, MapType)\n",
    "\n",
    "ensure_adls_auth()\n",
    "\n",
    "# Load Bronze paths\n",
    "P            = bronze_paths(\"feedback\")\n",
    "BRONZE_TABLE = P.table\n",
    "RAW_PATH     = raw_path(\"feedback\")\n",
    "BATCH_ID     = current_batch_id()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Define explicit JSON schema for performance & type safety.\n",
    "feedback_schema = StructType([\n",
    "    StructField(\"feedback_id\",        StringType(), True),\n",
    "    StructField(\"provider_id\",        StringType(), True),\n",
    "    StructField(\"timestamp\",          StringType(), True),\n",
    "    StructField(\"visit_id\",           StringType(), True),\n",
    "    StructField(\"satisfaction_score\", IntegerType(), True),\n",
    "    StructField(\"comments\",           StringType(), True),\n",
    "    StructField(\"source\",             StringType(), True),\n",
    "    StructField(\"tags\",               ArrayType(StringType()), True),\n",
    "    StructField(\"metadata\",           MapType(StringType(), StringType()), True),\n",
    "])\n",
    "\n",
    "# Register the schema for use in SQL (as a DDL string)\n",
    "schema_ddl = feedback_schema.simpleString().replace(\"struct<\", \"\").rstrip(\">\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Ensure Bronze Feedback table exists\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {BRONZE_TABLE} (\n",
    "  feedback_id        STRING,\n",
    "  provider_id        STRING,\n",
    "  timestamp          STRING,\n",
    "  visit_id           STRING,\n",
    "  satisfaction_score INT,\n",
    "  comments           STRING,\n",
    "  source             STRING,\n",
    "  tags               ARRAY<STRING>,\n",
    "  metadata_json      STRING,\n",
    "  _ingest_ts         TIMESTAMP,\n",
    "  _source_file       STRING,\n",
    "  _batch_id          STRING\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION '{P.bronze}'\n",
    "TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. Run batch operation\n",
    "#    COPY INTO scans the entire source path each run\n",
    "spark.sql(f\"\"\"\n",
    "COPY INTO {BRONZE_TABLE}\n",
    "FROM (\n",
    "  SELECT\n",
    "    CAST(feedback_id        AS STRING)            AS feedback_id,\n",
    "    CAST(provider_id        AS STRING)            AS provider_id,\n",
    "    CAST(timestamp          AS STRING)            AS timestamp,\n",
    "    CAST(visit_id           AS STRING)            AS visit_id,\n",
    "    CAST(satisfaction_score AS INT)               AS satisfaction_score,\n",
    "    CAST(comments           AS STRING)            AS comments,\n",
    "    CAST(source             AS STRING)            AS source,\n",
    "    CAST(tags               AS ARRAY<STRING>)     AS tags,\n",
    "    to_json(metadata)                              AS metadata_json,\n",
    "    current_timestamp()                            AS _ingest_ts,\n",
    "    input_file_name()                              AS _source_file,\n",
    "    '{BATCH_ID}'                                    AS _batch_id\n",
    "  FROM '{RAW_PATH}'\n",
    ")\n",
    "FILEFORMAT = JSON\n",
    "FORMAT_OPTIONS ('multiLine' = 'false')\n",
    "COPY_OPTIONS   ('mergeSchema' = 'false')\n",
    "\"\"\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4. Batch finished. Verify Bronze Feedback table and history\n",
    "df = spark.table(BRONZE_TABLE)\n",
    "print(f\"Bronze Feedback row count: {df.count():,}\")\n",
    "display(df.limit(5))\n",
    "show_history(P.bronze)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_feedback_copy_into",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
