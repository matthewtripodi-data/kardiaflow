{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# silver_encounters_scd1_stream.ipynb\n",
    "# SOURCE: Stream encounter records from Bronze (with Delta CDF) into Silver.\n",
    "# OUTPUT: `kardia_silver.silver_encounters`, updated incrementally.\n",
    "# TRIGGER: Continuously read incremental inserts and updates from Bronze Encounters table.\n",
    "\n",
    "from kflow.config import BRONZE_DB, SILVER_DB, bronze_table, silver_paths, CHANGE_TYPES\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load Silver paths\n",
    "S = silver_paths(\"encounters\")\n",
    "SRC_TABLE = bronze_table(\"encounters\")\n",
    "TGT_TABLE = S.table"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Ensure Silver Encounters table exists\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {TGT_TABLE} (\n",
    "      encounter_id       STRING  NOT NULL,\n",
    "      patient_id         STRING  NOT NULL,\n",
    "      START_TS           TIMESTAMP,\n",
    "      CODE               STRING,\n",
    "      DESCRIPTION        STRING,\n",
    "      REASONCODE         STRING,\n",
    "      REASONDESCRIPTION  STRING\n",
    "    ) USING DELTA\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. Define the upsert logic.\n",
    "#    For each batch, update or insert records by EncounterID from Delta CDF.\n",
    "#    `batch_df` is a static DF containing the latest new and updated rows from Bronze CDF.\n",
    "def upsert_to_silver(batch_df, batch_id):\n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "\n",
    "    (DeltaTable.forName(spark, TGT_TABLE)\n",
    "               .alias(\"t\")\n",
    "               .merge(\n",
    "                   batch_df.alias(\"s\"),\n",
    "                   \"t.encounter_id = s.encounter_id AND t.patient_id = s.patient_id\"\n",
    "               )\n",
    "               .whenMatchedUpdateAll()\n",
    "               .whenNotMatchedInsertAll()\n",
    "               .execute())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3a. Read new changes from the Bronze Encounters table.\n",
    "bronze_cdf = (\n",
    "    spark.readStream\n",
    "         .format(\"delta\")\n",
    "         .option(\"readChangeFeed\", \"true\")\n",
    "         .table(SRC_TABLE)\n",
    "         .filter(F.col(\"_change_type\").isin(*CHANGE_TYPES))\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3b. Enrich to seven-column Silver schema.\n",
    "silver_ready = (\n",
    "    bronze_cdf\n",
    "        .withColumnRenamed(\"ID\",      \"encounter_id\")\n",
    "        .withColumnRenamed(\"PATIENT\", \"patient_id\")\n",
    "        .withColumnRenamed(\"DATE\",    \"EVENT_DATE_STR\")\n",
    "\n",
    "        # Parse the raw date string into two formats:\n",
    "        # - EVENT_DATE (DateType)    - Useful for analytics\n",
    "        # - EVENT_TS (TimestampType) - If source starts sending real datetimes (future-proof)\n",
    "        .withColumn(\"EVENT_DATE\",      F.to_date(\"EVENT_DATE_STR\",      \"yyyy-MM-dd\"))\n",
    "        .withColumn(\"EVENT_TS\",        F.to_timestamp(\"EVENT_DATE_STR\", \"yyyy-MM-dd\"))\n",
    "\n",
    "        # Select final schema (exclude staging columns)\n",
    "        .selectExpr(\n",
    "            \"encounter_id\",\n",
    "            \"patient_id\",\n",
    "            \"EVENT_TS as START_TS\",\n",
    "            \"CODE\",\n",
    "            \"DESCRIPTION\",\n",
    "            \"REASONCODE\",\n",
    "            \"REASONDESCRIPTION\"\n",
    "        )\n",
    ")\n",
    "# NOTE: Timestamp is parsed as midnight in session time zone (UTC by default)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3c. Write to Silver table using foreachBatch + MERGE for upserts\n",
    "#     Process available data every 30 seconds\n",
    "query = (\n",
    "    silver_ready.writeStream\n",
    "                .foreachBatch(upsert_to_silver)\n",
    "                .option(\"checkpointLocation\", S.checkpoint)\n",
    "                .trigger(processingTime=\"30 seconds\")\n",
    "                .start()\n",
    ")\n",
    "print(f\"Stream silver_encounters started. Source: CDF({SRC_TABLE}), Sink: {TGT_TABLE}\")\n",
    "\n",
    "# NOTE: MERGE will write last-writer-wins"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7461780133328231,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver_encounters_scd1_stream",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
