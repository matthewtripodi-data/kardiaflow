{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3abf6d83-6f79-4bdc-adc3-c913389df770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# silver_encounters_scd1.ipynb\n",
    "# SOURCE: Reads encounter records from Bronze (with Delta CDF) into Silver.\n",
    "# OUTPUT: `kardia_silver.silver_encounters`, updated incrementally.\n",
    "# TRIGGER:\n",
    "# In batch mode: Reads available data and exits.\n",
    "# In stream mode: Runs continuously with 30s micro-batches.\n",
    "\n",
    "%pip install -q --no-deps --no-index --find-links=/dbfs/Shared/libs kflow\n",
    "from kflow.config import BRONZE_DB, SILVER_DB, bronze_table, silver_paths, CHANGE_TYPES\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load Silver paths\n",
    "S = silver_paths(\"encounters\")\n",
    "SRC_TABLE = bronze_table(\"encounters\")\n",
    "TGT_TABLE = S.table"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Mode widget & flags\n",
    "try:\n",
    "    dbutils.widgets.dropdown(\"mode\", \"batch\", [\"batch\", \"stream\"])\n",
    "except:\n",
    "    pass\n",
    "MODE = dbutils.widgets.get(\"mode\") if \"dbutils\" in globals() else \"batch\"\n",
    "IS_BATCH = (MODE == \"batch\")\n",
    "CHECKPOINT = f\"{S.checkpoint}/{MODE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "016cb326-066d-4d65-9a9f-c7384847d45b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Ensure Silver DB and Encounters table exists\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {S.db}\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {TGT_TABLE} (\n",
    "      encounter_id       STRING  NOT NULL,\n",
    "      patient_id         STRING  NOT NULL,\n",
    "      START_TS           TIMESTAMP,\n",
    "      CODE               STRING,\n",
    "      DESCRIPTION        STRING,\n",
    "      REASONCODE         STRING,\n",
    "      REASONDESCRIPTION  STRING\n",
    "    ) USING DELTA\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06d43d7a-5889-4024-baba-0e2e38f51372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Define the upsert logic.\n",
    "#    For each batch, update or insert records by EncounterID from Delta CDF.\n",
    "#    `batch_df` is a static DF containing the latest new and updated rows from Bronze CDF.\n",
    "def upsert_to_silver(batch_df, batch_id):\n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "\n",
    "    (DeltaTable.forName(spark, TGT_TABLE)\n",
    "               .alias(\"t\")\n",
    "               .merge(\n",
    "                   batch_df.alias(\"s\"),\n",
    "                   \"t.encounter_id = s.encounter_id AND t.patient_id = s.patient_id\"\n",
    "               )\n",
    "               .whenMatchedUpdateAll()\n",
    "               .whenNotMatchedInsertAll()\n",
    "               .execute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a0d2947-8e11-4546-a429-43811e506b22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3a. Read new changes from the Bronze Encounters table.\n",
    "bronze_cdf = (\n",
    "    spark.readStream\n",
    "         .format(\"delta\")\n",
    "         .option(\"readChangeFeed\", \"true\")\n",
    "         .table(SRC_TABLE)\n",
    "         .filter(F.col(\"_change_type\").isin(*CHANGE_TYPES))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c204c1b-f47d-460f-aad9-77add3128745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3b. Enrich to seven-column Silver schema.\n",
    "silver_ready = (\n",
    "    bronze_cdf\n",
    "        .withColumnRenamed(\"ID\",      \"encounter_id\")\n",
    "        .withColumnRenamed(\"PATIENT\", \"patient_id\")\n",
    "        .withColumnRenamed(\"DATE\",    \"EVENT_DATE_STR\")\n",
    "\n",
    "        # Parse the raw date string into two formats:\n",
    "        # - EVENT_DATE (DateType)    - Useful for analytics\n",
    "        # - EVENT_TS (TimestampType) - If source starts sending real datetimes (future-proof)\n",
    "        .withColumn(\"EVENT_DATE\",      F.to_date(\"EVENT_DATE_STR\",      \"yyyy-MM-dd\"))\n",
    "        .withColumn(\"EVENT_TS\",        F.to_timestamp(\"EVENT_DATE_STR\", \"yyyy-MM-dd\"))\n",
    "\n",
    "        # Select final schema (exclude staging columns)\n",
    "        .selectExpr(\n",
    "            \"encounter_id\",\n",
    "            \"patient_id\",\n",
    "            \"EVENT_TS as START_TS\",\n",
    "            \"CODE\",\n",
    "            \"DESCRIPTION\",\n",
    "            \"REASONCODE\",\n",
    "            \"REASONDESCRIPTION\"\n",
    "        )\n",
    ")\n",
    "# NOTE: Timestamp is parsed as midnight in session time zone (UTC by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d7bb5a5-b3f1-4784-9585-90f957e29aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3c. Write to Silver table using foreachBatch + MERGE for upserts\n",
    "#     Process available data every 30 seconds\n",
    "writer = (\n",
    "    silver_ready.writeStream\n",
    "                .foreachBatch(upsert_to_silver)\n",
    "                .option(\"checkpointLocation\", CHECKPOINT)\n",
    ")\n",
    "\n",
    "if IS_BATCH:\n",
    "    q = writer.trigger(availableNow=True).start()\n",
    "    print(f\"[demo] Draining CDF -> {TGT_TABLE} (checkpoint={CHECKPOINT}) â€¦\")\n",
    "    q.awaitTermination()\n",
    "else:\n",
    "    q = writer.trigger(processingTime=\"30 seconds\").start()\n",
    "    print(f\"[live] Continuous 30s CDF upserts to {TGT_TABLE} (checkpoint={CHECKPOINT})\")\n",
    "\n",
    "# NOTE: MERGE will write last-writer-wins"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7461780133328231,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_encounters_scd1_stream",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
