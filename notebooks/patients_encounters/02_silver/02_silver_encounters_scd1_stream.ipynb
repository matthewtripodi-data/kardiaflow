{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2284c3f9-11eb-4fae-8f5c-b366141ba286",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 02_silver_encounters_transform.ipynb\n",
    "# SOURCE: Stream encounter records from Bronze (with Delta CDF) into Silver.\n",
    "# OUTPUT: `kardia_silver.silver_encounters`, updated incrementally.\n",
    "# TRIGGER: Continuously read incremental inserts and updates from Bronze Encounters table.\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Table paths\n",
    "SILVER_DB = \"kardia_silver\"\n",
    "BRONZE_TABLE = \"kardia_bronze.bronze_encounters\"\n",
    "SILVER_TABLE = f\"{SILVER_DB}.silver_encounters\"\n",
    "CHECKPOINT_PATH = \"dbfs:/kardia/_checkpoints/silver_encounters\"\n",
    "\n",
    "# CDF event types we care about: insert and update_postimage only\n",
    "CHANGE_TYPES = [\"insert\", \"update_postimage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ca2697b-a5ee-47e4-ac4f-8c5f12623348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Ensure the Silver DB and Silver Encounters table exist.\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {SILVER_DB}\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {SILVER_TABLE} (\n",
    "      encounter_id       STRING  NOT NULL,\n",
    "      patient_id         STRING  NOT NULL,\n",
    "      START_TS           TIMESTAMP,\n",
    "      CODE               STRING,\n",
    "      DESCRIPTION        STRING,\n",
    "      REASONCODE         STRING,\n",
    "      REASONDESCRIPTION  STRING\n",
    "    ) USING DELTA\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c03289a4-7467-421d-b13e-d27cc3ff16b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Define the upsert logic.\n",
    "#    For each batch, update or insert records by EncounterID from Delta CDF.\n",
    "#    `batch_df` is a static DF containing the latest new and updated rows from Bronze CDF.\n",
    "def upsert_to_silver(batch_df, batch_id):\n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "\n",
    "    DeltaTable.forName(spark, SILVER_TABLE) \\\n",
    "      .alias(\"target\") \\\n",
    "      .merge(\n",
    "         batch_df.alias(\"source\"),\n",
    "         # merge on *both* keys—although encounter_id is your primary,\n",
    "         # including patient_id doubles down on correctness\n",
    "         \"target.encounter_id = source.encounter_id \"\n",
    "         \"AND target.patient_id = source.patient_id\"\n",
    "      ) \\\n",
    "      .whenMatchedUpdateAll() \\\n",
    "      .whenNotMatchedInsertAll() \\\n",
    "      .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7171933-7a52-47cf-aeed-5ecf93f07954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3a. Read new changes from the Bronze Encounters table.\n",
    "bronze_cdf = (\n",
    "    spark.readStream\n",
    "         .format(\"delta\")\n",
    "         .option(\"readChangeFeed\", \"true\")\n",
    "         .table(BRONZE_TABLE)\n",
    "         .filter(F.col(\"_change_type\").isin(*CHANGE_TYPES))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c1f93c3-7629-4045-a283-36b42f28cb44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3b. Enrich to seven-column Silver schema.\n",
    "silver_ready = (\n",
    "    bronze_cdf\n",
    "        .withColumnRenamed(\"ID\",      \"encounter_id\")\n",
    "        .withColumnRenamed(\"PATIENT\", \"patient_id\")\n",
    "        .withColumnRenamed(\"DATE\",    \"EVENT_DATE_STR\")\n",
    "\n",
    "        # Parse the raw date string into two formats:\n",
    "        # - EVENT_DATE (DateType)    - Useful for analytics\n",
    "        # - EVENT_TS (TimestampType) - If source starts sending real datetimes (future-proof)\n",
    "        .withColumn(\"EVENT_DATE\",      F.to_date(\"EVENT_DATE_STR\",      \"yyyy-MM-dd\"))\n",
    "        .withColumn(\"EVENT_TS\",        F.to_timestamp(\"EVENT_DATE_STR\", \"yyyy-MM-dd\"))\n",
    "\n",
    "        # Select final schema (exclude staging columns)\n",
    "        .selectExpr(\n",
    "            \"encounter_id\",\n",
    "            \"patient_id\",\n",
    "            \"EVENT_TS as START_TS\",\n",
    "            \"CODE\",\n",
    "            \"DESCRIPTION\",\n",
    "            \"REASONCODE\",\n",
    "            \"REASONDESCRIPTION\"\n",
    "        )\n",
    ")\n",
    "# NOTE: Timestamp is parsed as midnight in session time zone (UTC by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41eb38a3-1d08-4b5d-8410-229280786432",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3c. Write to Silver table using foreachBatch + MERGE for upserts\n",
    "#     Process available data every 30 seconds\n",
    "query = (\n",
    "    silver_ready.writeStream\n",
    "                .foreachBatch(upsert_to_silver)\n",
    "                .option(\"checkpointLocation\", CHECKPOINT_PATH)\n",
    "                .trigger(processingTime=\"30 seconds\")\n",
    "                .start()\n",
    ")\n",
    "\n",
    "displayHTML(\"\"\"\n",
    "    <div style='color:green; font-weight:bold;'>\n",
    "        Stream started: silver_encounters<br>\n",
    "        Trigger: every 30 sec • Source: CDF from bronze_encounters\n",
    "    </div>\n",
    "    \"\"\"\n",
    ")\n",
    "query.awaitTermination()\n",
    "\n",
    "# NOTE: No deduplication needed. Each EncounterID appears at most once per micro-batch since encounters are rarely updated."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4880559092059857,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver_encounters_scd1_stream",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
