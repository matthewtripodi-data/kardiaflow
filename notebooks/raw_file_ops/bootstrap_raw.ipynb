{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab25cf31-ffa9-45d3-9c8e-2e3196dfb72c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# raw_file_ops/bootstrap_raw.ipynb\n",
    "from kflow.config import raw_path, adls_raw_path\n",
    "from kflow.display_utils import banner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f68dad1-740f-45e5-82ab-07b36b3652e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Global Delta defaults (apply to *new* Delta tables)\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.autoOptimize.autoCompact\",  \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", \"true\")\n",
    "\n",
    "# Enable ${var} substitution in pure SQL cells\n",
    "spark.conf.set(\"spark.sql.variable.substitute\", \"true\")\n",
    "\n",
    "# Business knobs (“magic numbers”)\n",
    "spark.conf.set(\"kflow.rapid_fire.threshold\", 5)\n",
    "spark.conf.set(\"kflow.claims.hourly.time_col\", \"_ingest_ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37f6e142-b249-4c95-a863-cf098d5a6d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Databases – create once & tag with DBPROPERTIES\n",
    "db_props = \"\"\"\n",
    "  'delta.autoOptimize.optimizeWrite'='true',\n",
    "  'delta.autoOptimize.autoCompact' ='true'\n",
    "\"\"\"\n",
    "\n",
    "for db in [\"kardia_bronze\",\"kardia_silver\",\"kardia_gold\",\"kardia_validation\"]:\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db}\")\n",
    "    spark.sql(f\"ALTER DATABASE {db} SET DBPROPERTIES ({db_props})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4253f732-118b-4ab2-a0ba-0abcc64f14f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. ADLS auth (providers & feedback). Run once per cluster/session.\n",
    "ADLS_ACCOUNT = \"kardiaadlsdemo\"\n",
    "SUFFIX       = \"core.windows.net\"\n",
    "sas_token    = dbutils.secrets.get(\"kardia\",\"adls_raw_sas\").lstrip('?')\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{ADLS_ACCOUNT}.dfs.{SUFFIX}\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{ADLS_ACCOUNT}.dfs.{SUFFIX}\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{ADLS_ACCOUNT}.dfs.{SUFFIX}\", sas_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1bb70aa-01f1-4c4f-b47c-378ac1a7705c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Seed raw dirs with first sample files (run once per env)\n",
    "UPLOADS_DIR = \"dbfs:/FileStore/tables/\"   # where you manually uploaded seed files\n",
    "\n",
    "INITIAL_FILES = {\n",
    "    # DBFS-backed\n",
    "    \"patients\":   (\"patients_part_1.csv\",    raw_path(\"patients\")),\n",
    "    \"encounters\": (\"encounters_part_1.avro\", raw_path(\"encounters\")),\n",
    "    \"claims\":     (\"claims_part_1.parquet\",  raw_path(\"claims\")),\n",
    "    # ADLS-backed\n",
    "    \"providers\":  (\"providers_part_1.tsv\",   adls_raw_path(\"providers\")),\n",
    "    \"feedback\":   (\"feedback_part_1.jsonl\",  adls_raw_path(\"feedback\")),\n",
    "}\n",
    "# Helpers\n",
    "def list_names(path: str):\n",
    "    try:\n",
    "        return [f.name for f in dbutils.fs.ls(path)]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    # dbfs mkdirs work; abfss doesn't need it\n",
    "    if path.startswith(\"dbfs:/\"):\n",
    "        try:\n",
    "            dbutils.fs.mkdirs(path)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def copy_if_missing(src_dir: str, dest_dir: str, fname: str):\n",
    "    src, dst = src_dir + fname, dest_dir + fname\n",
    "    if fname in list_names(dest_dir):\n",
    "        print(f\"Skipped (already exists): {dst}\")\n",
    "        return\n",
    "    try:\n",
    "        dbutils.fs.cp(src, dst)\n",
    "        print(f\"Bootstrapped: {fname} → {dst}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to copy {fname}: {e}\")\n",
    "\n",
    "def bootstrap_initial_files(upload_dir: str, files: dict[str, tuple[str, str]]):\n",
    "    # Ensure DBFS dirs\n",
    "    for _, dest in files.values():\n",
    "        ensure_dir(dest)\n",
    "    # Copy once\n",
    "    uploaded = set(list_names(upload_dir))\n",
    "    for ds, (fname, dest_dir) in files.items():\n",
    "        if fname not in uploaded:\n",
    "            print(f\"[{ds}] Skipped – {fname} not found in {upload_dir}\")\n",
    "            continue\n",
    "        copy_if_missing(upload_dir, dest_dir, fname)\n",
    "\n",
    "bootstrap_initial_files(UPLOADS_DIR, INITIAL_FILES)\n",
    "banner(\"Bootstrap complete\", ok=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bootstrap_raw",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
