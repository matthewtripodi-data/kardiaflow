{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# raw_file_ops/bootstrap_raw.ipynb\n",
    "# 1. Install helpers once per cluster/session (optional if on repo cluster)\n",
    "%pip install -q git+https://github.com/okv627/KardiaFlow@main#subdirectory=src\n",
    "\n",
    "from kflow.config import raw_path, adls_raw_path\n",
    "from kflow.display_utils import banner"
   ],
   "id": "90dfd4e610901305"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. Global Delta defaults (apply to *new* Delta tables)\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.autoOptimize.autoCompact\",  \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", \"true\")\n",
    "\n",
    "# Enable ${var} substitution in pure SQL cells\n",
    "spark.conf.set(\"spark.sql.variable.substitute\", \"true\")\n",
    "\n",
    "# Business knobs (“magic numbers”)\n",
    "spark.conf.set(\"kflow.rapid_fire.threshold\", 5)\n",
    "spark.conf.set(\"kflow.claims.hourly.time_col\", \"_ingest_ts\")"
   ],
   "id": "4a9f3c2714e291"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Databases – create once & tag with DBPROPERTIES\n",
    "db_props = \"\"\"\n",
    "  'delta.autoOptimize.optimizeWrite'='true',\n",
    "  'delta.autoOptimize.autoCompact' ='true'\n",
    "\"\"\"\n",
    "\n",
    "for db in [\"kardia_bronze\",\"kardia_silver\",\"kardia_gold\",\"kardia_validation\"]:\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db}\")\n",
    "    spark.sql(f\"ALTER DATABASE {db} SET DBPROPERTIES ({db_props})\")"
   ],
   "id": "6764f10d85de2223"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4. ADLS auth (providers & feedback). Run once per cluster/session.\n",
    "ADLS_ACCOUNT = \"kardiaadlsdemo\"\n",
    "SUFFIX       = \"core.windows.net\"\n",
    "sas_token    = dbutils.secrets.get(\"kardia\",\"adls_raw_sas\").lstrip('?')\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{ADLS_ACCOUNT}.dfs.{SUFFIX}\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{ADLS_ACCOUNT}.dfs.{SUFFIX}\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{ADLS_ACCOUNT}.dfs.{SUFFIX}\", sas_token)"
   ],
   "id": "2c5c10fcfa9d6c90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 5. Seed raw dirs with first sample files (run once per env)\n",
    "UPLOADS_DIR = \"dbfs:/FileStore/tables/\"   # where you manually uploaded seed files\n",
    "\n",
    "INITIAL_FILES = {\n",
    "    # DBFS-backed\n",
    "    \"patients\":   (\"patients_part_1.csv\",    raw_path(\"patients\")),\n",
    "    \"encounters\": (\"encounters_part_1.avro\", raw_path(\"encounters\")),\n",
    "    \"claims\":     (\"claims_part_1.parquet\",  raw_path(\"claims\")),\n",
    "    # ADLS-backed\n",
    "    \"providers\":  (\"providers_part_1.tsv\",   adls_raw_path(\"providers\")),\n",
    "    \"feedback\":   (\"feedback_part_1.jsonl\",  adls_raw_path(\"feedback\")),\n",
    "}\n",
    "# Helpers\n",
    "def list_names(path: str):\n",
    "    try:\n",
    "        return [f.name for f in dbutils.fs.ls(path)]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    # dbfs mkdirs work; abfss doesn't need it\n",
    "    if path.startswith(\"dbfs:/\"):\n",
    "        try:\n",
    "            dbutils.fs.mkdirs(path)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def copy_if_missing(src_dir: str, dest_dir: str, fname: str):\n",
    "    src, dst = src_dir + fname, dest_dir + fname\n",
    "    if fname in list_names(dest_dir):\n",
    "        print(f\"Skipped (already exists): {dst}\")\n",
    "        return\n",
    "    try:\n",
    "        dbutils.fs.cp(src, dst)\n",
    "        print(f\"Bootstrapped: {fname} → {dst}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to copy {fname}: {e}\")\n",
    "\n",
    "def bootstrap_initial_files(upload_dir: str, files: dict[str, tuple[str, str]]):\n",
    "    # Ensure DBFS dirs\n",
    "    for _, dest in files.values():\n",
    "        ensure_dir(dest)\n",
    "    # Copy once\n",
    "    uploaded = set(list_names(upload_dir))\n",
    "    for ds, (fname, dest_dir) in files.items():\n",
    "        if fname not in uploaded:\n",
    "            print(f\"[{ds}] Skipped – {fname} not found in {upload_dir}\")\n",
    "            continue\n",
    "        copy_if_missing(upload_dir, dest_dir, fname)\n",
    "\n",
    "bootstrap_initial_files(UPLOADS_DIR, INITIAL_FILES)\n",
    "banner(\"Bootstrap complete\", ok=True)"
   ],
   "id": "d7d84cbd7d7e878c"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
