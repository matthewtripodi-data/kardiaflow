# QualiFlow  

## A Framework for Visible, Portable Data Trust  

*QualiFlow is a data trust framework that embeds portable, evidence-based metadata directly into data products — making freshness, quality, privacy, and lineage transparent to every stakeholder.*

**Goal:**  
To democratize data quality metrics by making a **tiered view of evidence**  
available to all stakeholders—at the level of depth they need—bundled with the  
data artifact and delivered to the consumption point as one package.

---

**QualiFlow defines a display protocol** that defines:

- What metadata to store  
- How to score and evaluate it  
- How to present it consistently wherever the data is served

This shifts **data reliability** from a backend/internal concern to a  
**first-class component of user experience**.

---

### Why It Matters

Today, metadata and quality checks often live in disconnected systems—  
observability platforms, data catalogs, pipeline logs, etc.

With **QualiFlow**, the **proof artifacts travel with the data itself**.  
The result is a single, transparent artifact:  
the **evidence bundle and its visual indicators**.

Stakeholders get:

- A simple **indicator of trust**
- The ability to **self-serve deeper context** if needed

---

### Key Features

- **Readable, Simplified Trust Signals:**  
  Instead of random metadata like null percentages, QualiFlow emphasizes  
  clean scores, quality grades, and visual badges

- **Reproducibility Fingerprint:**  
  A unique ID representing the exact code and data state—  
  useful for engineers who may need to fetch logs or rerun the pipeline

- **Privacy Badge with Detail:**  
  Goes beyond “compliant” to show **what was done** to protect sensitive data

> Example:  
> _“Privacy: Masked 2 fields (Name, SSN) under Policy HIPAA-PrivacyRule-1,  
> resulting in 12% of values masked.”_

---

# QualiFlow Has Two Main Layers

---

## Evidence Bundle (Machine Layer)

> **This is the machine-readable layer.**  
> Each evidence bundle is generated by **QualiFlow** and **encoded in the Evidence Bundle Protocol (EBP)** — a machine-readable JSON standard that captures freshness, quality, lineage, privacy, and reproducibility metadata.

A **machine-readable package of metadata** that travels with the data — a compact JSON  
envelope of proof attached to a dataset or result.

The evidence bundle can be stored in a metadata repository or table  
(e.g., a `gold_qf_evidence` table in a data warehouse, keyed by `dataset_id` and `timestamp`).

Each time the data is refreshed:

- A new JSON bundle is generated and stored
- The dataset carries a reference to it (e.g., an ID or fingerprint)

### The Evidence Bundle Includes:

- **Origin & Lineage:**  
  Where did the data come from (e.g., upstream datasets, source systems)?  
  How was it produced (e.g., pipeline job ID, code commit/version)?

- **Freshness / Latency:**  
  - When was the data ingested?  
  - When was it last updated?  
  - How long did the pipeline take?  
  - How does current latency compare to the SLA?

- **Quality Checks:**  
  Summary of validation results, number of tests passed/failed, and a data quality score  
  _(e.g., “null percentage = 0.2%” or “98% of values within expected range”)_

- **Contract / Schema Info:**  
  What schema version does this data adhere to?  
  Includes a schema hash or version ID.  
  _(e.g., “We promised the data would look like X — is this using version X, and was that respected?”)_

- **Privacy / Compliance Status:**  
  Was sensitive data handled properly?  
  Which columns are masked or tokenized?  
  What percentage of data was masked?

- **Reproducibility Fingerprint:**  
  A unique ID representing the exact dataset and the code that produced it,  
  derived from code version + input data snapshot identifiers.  
  Useful for engineers needing to fetch logs or rerun the pipeline.

- **Timestamp / Ownership:**  
  When was the evidence bundle generated, and by whom or what?  
  _(e.g., “Data product owner: data-eng@company.com, generated at 2025-07-22 14:33 UTC”)_

---

### Displaying the Evidence

The UI renderer (e.g., in a BI or dashboard tool) consumes the evidence JSON  
to display user-friendly proof indicators.

#### For example:

- `freshness.latency_seconds = 20` vs SLA `900` → **Freshness: Green – updated 20s ago**
- `quality.tests_failed = 0`, `dq_score ≈ 98.7` → **Checkmark or “Quality: A”**
- `contract.status = OK` → **Label: “Schema v3.1”**
- `privacy.masked_fields` is non-empty → **Lock icon displayed**

---

### Key Concept

For any data artifact, you can retrieve a **JSON payload** (like a **nutritional label**)  
containing all the standardized metadata listed above.

By standardizing keys and semantics, any tool that adopts QualiFlow can both **produce** and **consume**  
this evidence in a consistent, interoperable way.

---

## Proof Indicators (Human Layer)

> **This is the human-facing UI layer.**  
> It distills metadata from the evidence bundle into visual indicators, badges, and contextual explanations.

These are the **UI elements** that surface the evidence to users.  
Rather than dumping raw JSON, QualiFlow defines how to present the highlights of the evidence bundle  
in context — using **progressive disclosure**.

### Examples include:

- **Trust Badge:**  
  Badge or icon next to a metric with a status indicator (e.g., green if all tests pass, yellow if warnings, etc.)

- **Freshness Badge:**  
  A label like `"Fresh (15m)"` if the data is 15 minutes old, or `"Stale"` / `"Updated 2h ago"`.  
  Every query includes a freshness receipt showing whether the dataset met SLOs.

- **Quality Score:**  
  `"Data Quality: 98% (12/12 tests passed)"`

- **"Show Lineage" Link or Icon**

- **Validation Details / Proof Report:**  
  A panel or page showing the full validation report — e.g.,  
  Great Expectations Data Docs, data quality metrics, or detailed provenance info.

- **Privacy Indicator:**  
  Icon or text if privacy actions were applied (e.g.,  
  `"PII masked: Yes (2 fields masked)"`), with the ability to click and view which fields were masked.

- **Reproducibility Fingerprint Display:**  
  A short hash or run ID shown in an info panel or tooltip (e.g., `"Run ID: 4f67e2"`),  
  useful for engineers to trace the exact pipeline run and code version that produced the data.

QualiFlow indicators are **layered** to suit different roles:

- **Executives / Casual Consumers:**  
  See simple green/yellow/red badges

- **Analysts:**  
  Can click to view more details (e.g., test results, schema versions, recent changes)

- **Engineers:**  
  Can drill down to the raw evidence JSON

---

## Implementation of Proof on Display

> **Note:** Not every phase is required — implementation depends on your organization's needs.

### Phase 1 – Inventory & Gap Analysis

Start by listing the data quality metadata you already collect and where it resides.  
Examples include ingestion timestamps, validation results stored in QA tables, pipeline logs,  
transaction history, or schema versions tracked in code repositories.

Then assess:

- Which QualiFlow evidence fields you can populate with existing data  
- Which fields will require new instrumentation

---

### Phase 2 – Generate Evidence Bundles

Build a mechanism to generate a JSON evidence bundle whenever data is refreshed.

For example, a `dbt` model or notebook at the end of a pipeline can:

- Gather metrics (e.g., test pass count, freshness, data quality score)
- Assemble them into a JSON structure
- Write that JSON to a QualiFlow evidence table
- Attach a reference to the dataset itself, such as:

> **In Databricks Delta**, you can store the fingerprint or reference ID as a column  
like `_qf_fingerprint` in each Gold table. Dashboards can then join or look up  
the associated evidence by this ID.

#### Example:

When refreshing `gold_patient_lifecycle`, the pipeline would:

- Compute freshness (e.g., max `_ingest_ts` from source tables)
- Run validations (e.g., via Great Expectations or custom logic)
- Calculate a data quality score from the validation results
- Capture the Delta table version or transaction ID for lineage
- Hash the current dbt git SHA + Delta versions to generate a fingerprint
- Write the evidence JSON to a table and update the `_qf_fingerprint` column in the Gold table

---

### Phase 3 – Display Layer Integration

In BI tools or notebooks, integrate display widgets that surface QualiFlow evidence.

Options include:

- Custom indicators beside each chart that query the latest evidence and render a color-coded trust badge
- Notebook utilities that print a mini "proof card" at the top of an analysis
- API methods that return the full evidence bundle for programmatic access

Every indicator should:

- Answer a clear trust-related question (e.g., "Is this fresh?" or "Was anything masked?")
- Provide progressive disclosure — e.g., a privacy badge should be clickable to reveal what was masked and why,  
  possibly linking to internal documentation or policy wikis

---

### Phase 4 – Trust SLOs & Automatic Receipts

Once QualiFlow is in place, define **Service Level Objectives (SLOs)** for data trust.  
Examples include:

- **Freshness:** less than 1 hour  
- **Data Quality Score:** ≥ 95  
- **Lineage Coverage:** ≥ 90% of upstream sources documented

Then:

- Automate evaluation of these SLOs during each pipeline run
- Add the evaluation result to the evidence bundle (`slo_status: PASS` / `FAIL`)
- Escalate visual indicators if SLOs fail (e.g., yellow badge or red alert)

Additionally, generate a **“trust receipt”** — a human-readable snapshot or PDF report  
summarizing the evidence bundle for auditing purposes.  
For example:

> "On July 22, the dataset X was refreshed. All tests passed, freshness SLA met, etc.  
> — Signed by pipeline at 14:33 UTC."

This report can be archived and used in regulated environments to demonstrate compliance.

---

### Phase 5 – Packaging & Reuse

Create reusable components to make QualiFlow adoption easy across teams:

- Utility functions to generate JSON bundles from standard inputs
- Dashboard widget templates for trust badges
- Internal or open-source libraries that define QualiFlow formats and renderers

---

### Summary

Implementing QualiFlow means tightly integrating **data operations** with **data user experience**.  
It requires collaboration between:

- Data Engineering
- DevOps / Platform Engineering
- BI & Dashboard Designers

But the payoff is significant: a visible, reliable, and standardized **trust layer** for data.

---

## Potential Challenges with QualiFlow

While QualiFlow can offer major benefits, it's not universally appropriate. Some key considerations include:

- **Overhead for Small Teams:**  
  For tightly-knit or early-stage teams, QualiFlow may feel like over-engineering. If trust already exists, additional indicators may offer limited value.

- **Visual Clutter or Misuse:**  
  Poorly designed QualiFlow implementations can overwhelm dashboards with unnecessary icons. Simplicity and intuitive UI are critical to avoid confusion or “badge fatigue.”

- **False Sense of Security:**  
  If the QualiFlow layer malfunctions or isn’t kept up to date, it can mislead users—creating trust in faulty data. QualiFlow must itself be trustworthy and tested.

- **Cultural Buy-In Required:**  
  Stakeholders unfamiliar with data quality metrics may find the system excessive or unclear. Success depends on education, trust-building, and data maturity.

- **Engineering Effort:**  
  Setting up QualiFlow requires pipeline instrumentation, metadata management, and front-end integration—especially challenging in BI tools with limited customization.

---

## Conclusion

QualiFlow shines in **complex, distributed, and high-stakes environments** where transparency, auditability, and trust are essential. It may be excessive for small or informal projects, but when implemented thoughtfully, it becomes a lightweight, visual **data warranty system**.

Ultimately, **QualiFlow is not just about quality checks—it’s about communicating quality** in a consistent, user-facing way. When done right, it becomes a powerful trust signal across the data lifecycle.

---

## Sample Evidence Bundle

Below is an example of a QualiFlow-compliant evidence bundle — a machine-readable JSON object  
attached to a dataset at the time of pipeline execution. It includes lineage, freshness, quality,  
schema contract status, privacy details, and a reproducibility fingerprint.

```json
{
  "dataset_id": "gold_patient_lifecycle",
  "lineage": {
    "parents": [
      "silver_encounters_enriched_v5",
      "silver_patients_v3"
    ],
    "job_run_id": "pipeline_run_2025-07-22T14:33:02Z",
    "code_version": "git:main@a1b2c3d"
  },
  "freshness": {
    "data_timestamp": "2025-07-22T14:32:50Z",
    "loaded_at": "2025-07-22T14:33:10Z",
    "latency_seconds": 20,
    "sla": "15m",
    "status": "OK"
  },
  "quality": {
    "tests_passed": 28,
    "tests_failed": 0,
    "dq_score": 98.7,
    "dimensions": {
      "completeness": 1.0,
      "accuracy": 0.987,
      "consistency": 0.99
    }
  },
  "contract": {
    "schema_version": "3.1",
    "schema_hash": "sha256:abcdef123456...",
    "contract_id": "encounters_contract_v2",
    "status": "OK"
  },
  "privacy": {
    "masked_fields": ["SSN", "LAST_NAME"],
    "masking_policy_id": "PHI_Mask_v1",
    "masked_pct": 12.4
  },
  "fingerprint": "qf:sha256:4f67e2a8c9...",
  "owner": "data-eng-team@mycompany.com",
  "generated_at": "2025-07-22T14:33:15Z"
}

---