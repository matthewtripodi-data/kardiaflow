# Kardiaflow Infrastructure Deployment Guide

This folder contains the infrastructure-as-code (IaC) scripts for deploying and
tearing down the minimal Kardiaflow development environment in Azure using Bicep
and the Azure CLI.


## What It Deploys

KardiaFlow can be deployed in two modes depending on your Databricks workspace tier:

| Mode      | Bicep File                | Workspace Tier | Notes                                  |
|-----------|---------------------------|----------------|----------------------------------------|
| Standard  | `deploy_standard.bicep`   | Databricks Standard SKU | Minimal cost; no Unity Catalog features |
| Premium   | `deploy_premium.bicep`    | Databricks Premium SKU  | Enables Unity Catalog, dashboards, etc. |

---

### Resource Summary

| Resource Group                     | Resources Created                                                                                                                | Notes                                                                                     | Cost Guidance                                                                                                   |
|-----------------------------------|----------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|
| **kardiaâ€‘rgâ€‘dev**                 | â€¢ `kardiaâ€‘dbx` (Databricks workspace)<br>â€¢ `kardiaadlsdemo` (ADLS Gen2 account, `raw` container)                                 | Control objects defined in Bicep script                                                   | â€¢ Workspace is control-plane only (no cost while clusters are off).<br>â€¢ Storage billed by usage (LRS hot tier)<br>â€¢ Premium DBU rate may double job run costs (e.g., +$0.30â€“$0.90/day)    |
| **kardiaâ€‘dbxâ€‘managed** (autogenerated) | â€¢ `dbmanagedidentity`<br>â€¢ `dbstorageâ€¦`<br>â€¢ `workersâ€‘vnet`<br>â€¢ `workersâ€‘sg`<br>â€¢ `unity-catalog-access-connector` <br><small>(Premium only)</small> | Auto-provisioned by Databricks when deploying a Premium workspace                         | â€¢ All resources are free except DBFS (`dbstorage`, usually < 5 GB).<br>â€¢ Access Connector is free               |
| **NetworkWatcherRG** (autogenerated)   | â€¢ `NetworkWatcher_<region>`                                                                                                     | Created by Azure when a VNet is created in a region                                       | Free                                                                                                            |

---

> Use `deploy_premium.bicep` **only when** Unity Catalog or dashboard features are required. The standard deployment avoids those capabilities for lower cost and simplicity.


## Deploy Instructions

Run these from your local terminal in the project root. Make sure you're logged into the correct Azure subscription.

**1. Load environment variables from .env**

```bash
source infra/.env
```

---

**2. Create the Azure resource group**

```bash
az group create --name "$RG" --location eastus
```

---

**3. Deploy infrastructure with Bicep (Databricks + ADLS)**

```bash
# Option A â€“ Standard (default, cheaper)
az deployment group create \
  --resource-group "$RG" \
  --template-file infra/bicep/deploy_standard.bicep \
  --name "$DEPLOY"
```

```bash
# Option B â€“ Premium (if needed for dashboards or Unity Catalog)
az deployment group create \
  --resource-group "$RG" \
  --template-file infra/bicep/deploy_premium.bicep \
  --name "$DEPLOY"
```

---

**4. Generate a Databricks Personal Access Token (PAT)**

(Databricks UI â†’ Settings â†’ Developer â†’ Generate New Token)

---

**5. Add your PAT to .env as `DATABRICKS_PAT=your_token_here`**

---

**6. Configure the Databricks CLI profile**

```bash
url=$(az databricks workspace show -g "$RG" -n "$WORKSPACE" --query "workspaceUrl" -o tsv)
databricks configure --profile "$PROFILE" --host "https://$url" --token <<< "${DATABRICKS_PAT}"$'\n'
```

---

**7. Run gen_sas.sh to auto-generate and store the ADLS SAS token in Databricks**

```bash
infra/deploy/gen_sas.sh
```

---

**8. Build and upload the wheel to DBFS**

This installs the build tool, creates the .whl package from pyproject.toml, and uploads it to DBFS.

```bash
python -m pip install --upgrade build
python -m build

# pick the newest wheel
WHL=$(ls dist/kflow-*-py3-none-any.whl | tail -n 1)

# copy the versioned wheel to DBFS
databricks fs mkdirs dbfs:/Shared/libs
databricks fs cp "$WHL" dbfs:/Shared/libs/ --overwrite
```

> ðŸ’¡ **Why DBFS?**  
> Itâ€™s the simplest way to make local Python packages available to notebooks using `%pip install --no-index --find-links=...`.  
> Compatible with cost-efficient DBR 13.3 LTS and avoids issues with workspace paths or job-level libraries.

---

**9. Create or reset the batch job**

```bash
# To create from scratch
databricks jobs create --json '@pipelines/kardiaflow_full_run_batch.json' --profile kardia
```

```bash
# To reset with a new wheel or config
databricks jobs reset --json @pipelines/reset_kardiaflow_full_run_batch.json
```

---

**9. Tear down all provisioned resources**

```bash
./infra/deploy/teardown.sh
```

The teardown script script will:

- Deletes the Databricks workspace (automatically removes the managed RG)
- Deletes the main resource group (kardia-rg-dev)
- Print a confirmation message
- Resources disappear within 2â€“5 minutes.

---

### When to Build a New Wheel

If you've made changes to the `kflow` package â€” such as editing any `.py` files inside the `kflow/` directory â€” follow these steps to deploy your updates:

1. Update the version number in `pyproject.toml`

`version = "0.2.6"` â†’ `version = "0.2.7"`


2. Clean old build artifacts (recommended for a fresh build):

```bash
rm -rf dist/*
```

3. Rebuild and re-upload the new wheel:

```bash
python -m build
WHL=$(ls dist/kflow-*-py3-none-any.whl | tail -n 1)
databricks fs cp "$WHL" dbfs:/Shared/libs/ --overwrite
```

This ensures all notebooks and job runs use the latest version and prevents caching issues with `%pip`.

---

### Dry-Run Deployment

To preview what the deployment will do without actually creating resources:

```bash
az deployment group what-if \
  --resource-group "$RG" \
  --template-file infra/bicep/deploy.bicep \
  --name "$DEPLOY"
```