# KardiaFlow Infrastructure Deployment Guide

This folder contains the infrastructure-as-code (IaC) scripts for deploying and tearing down the KardiaFlow development environment in Azure using Bicep and the Azure CLI.

---

## What It Deploys

KardiaFlow is designed for use with a **Databricks Premium-tier workspace**, enabling support for dashboards and future extensibility.

> ðŸ’¡ **Note:**  
> KardiaFlow does **not** use Unity Catalog or Delta Live Tables (DLT).  
> It relies on Delta Lake, Auto Loader, and Databricks Jobs for portability and low cost.

---

### Resource Summary

| Resource Group                     | Resources Created                                                                                                   | Notes                                                                                     | Cost Guidance                                                                                                   |
|-----------------------------------|---------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|
| **kardiaâ€‘rgâ€‘dev**                 | â€¢ `kardiaâ€‘dbx` (Databricks workspace)<br>â€¢ `kardiaadlsdemo` (ADLS Gen2 account, `lake` container)                   | Created via Bicep                                                                         | â€¢ Workspace control-plane is free while clusters are off<br>â€¢ Storage billed by usage (LRS hot tier) |
| **kardiaâ€‘dbxâ€‘managed** (autogenerated) | â€¢ `dbmanagedidentity`<br>â€¢ `dbstorageâ€¦`<br>â€¢ `workersâ€‘vnet`<br>â€¢ `workersâ€‘sg`<br>â€¢ `unity-catalog-access-connector` | Provisioned automatically by Databricks                                                  | â€¢ All resources are free except `dbstorage` (minimal DBFS usage)                                               |
| **NetworkWatcherRG** (autogenerated)   | â€¢ `NetworkWatcher_<region>`                                                                                         | Created by Azure when a VNet is provisioned                                              | Free                                                                                                            |

---

## Deploy Instructions

Run these from your local terminal in the project root. Make sure you're logged into the correct Azure subscription.

**1. Load environment variables from .env**

```bash
source infra/.env
az account set --subscription "$SUB"
```

---

**2. Create the Azure resource group**

```bash
az group create --name "$RG" --location eastus
```

---

**3. Deploy infrastructure with Bicep (Databricks + ADLS)**

```bash
az deployment group create \
  --resource-group "$RG" \
  --template-file infra/bicep/deploy_premium.bicep \
  --name "$DEPLOY"
```

---

**4. Generate a Databricks Personal Access Token (PAT) in the UI**

(Settings â†’ Developer â†’ Generate New Token)

Add it to infra/.env as ```DATABRICKS_PAT=...```

---

**5. Authenticate once (now that PAT exists)**

```bash
source infra/.env
source infra/deploy/auth.sh
```

---

**6. Generate and store the ADLS SAS token in Databricks**

```bash
infra/deploy/gen_sas.sh
```

---

**7. Build and upload the wheel to DBFS**

This installs the build tool, creates the .whl package from pyproject.toml, and uploads it to DBFS.

```bash
python -m pip install --upgrade build
python -m build
WHL=$(ls dist/kflow-*-py3-none-any.whl | tail -n 1)

databricks fs mkdirs dbfs:/Shared/libs
databricks fs cp "$WHL" dbfs:/Shared/libs/ --overwrite
```

---

**7. Create Medallion Layer Folders**

Run ```notebooks/99_utilities/bootstrap_dir_adls.ipynb``` to create /kardia/{bronze,silver,gold} in ADLS:

---

**8. Create or reset the full run batch job**

```bash
# To create from scratch
databricks jobs create --json @pipelines/jobs/kardiaflow_full_run_batch.json
```

```bash
# To reset with a new wheel or config
databricks jobs reset --json @pipelines/jobs/reset_kardiaflow_full_run_batch.json
```

---

**9. Tear down all provisioned resources**

```bash
./infra/deploy/teardown.sh
```

The teardown script script will:

- Deletes the Databricks workspace (automatically removes the managed RG)
- Deletes the main resource group (kardia-rg-dev)
- Print a confirmation message
- Resources disappear within 2â€“5 minutes.

---

### When to Build a New Wheel

If you've made changes to the `kflow` package â€” such as editing any `.py` files inside the `kflow/` directory â€” 
follow these steps to deploy your updates:

1. Update the version number in `pyproject.toml`

`version = "0.2.6"` â†’ `version = "0.2.7"`

2. Rebuild and re-upload the new wheel:

```bash
python -m build --wheel
WHL=$(ls -t dist/kflow-*-py3-none-any.whl | head -1)

databricks fs mkdirs dbfs:/Shared/libs
databricks fs cp "$WHL" dbfs:/Shared/libs/ --overwrite
```

This ensures all notebooks and job runs use the latest version and prevents caching issues with `%pip`.

---

### Dry-Run Deployment

To preview what the deployment will do without actually creating resources:

```bash
az deployment group what-if \
  --resource-group "$RG" \
  --template-file infra/bicep/deploy.bicep \
  --name "$DEPLOY"
```