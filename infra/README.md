# Kardiaflow Infrastructure Deployment Guide

This folder contains the infrastructure-as-code (IaC) scripts for deploying and
tearing down the minimal Kardiaflow development environment in Azure using Bicep
and the Azure CLI.


## What It Deploys

KardiaFlow can be deployed in two modes depending on your Databricks workspace tier:

| Mode      | Bicep File                | Workspace Tier | Notes                                  |
|-----------|---------------------------|----------------|----------------------------------------|
| Standard  | `deploy_standard.bicep`   | Databricks Standard SKU | Minimal cost; no Unity Catalog features |
| Premium   | `deploy_premium.bicep`    | Databricks Premium SKU  | Enables Unity Catalog, dashboards, etc. |

---

### Resource Summary

| Resource Group                     | Resources Created                                                                                                                | Notes                                                                                     | Cost Guidance                                                                                                   |
|-----------------------------------|----------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|
| **kardia‑rg‑dev**                 | • `kardia‑dbx` (Databricks workspace)<br>• `kardiaadlsdemo` (ADLS Gen2 account, `raw` container)                                 | Control objects defined in Bicep script                                                   | • Workspace is control-plane only (no cost while clusters are off).<br>• Storage billed by usage (LRS hot tier)<br>• Premium DBU rate may double job run costs (e.g., +$0.30–$0.90/day)    |
| **kardia‑dbx‑managed** (autogenerated) | • `dbmanagedidentity`<br>• `dbstorage…`<br>• `workers‑vnet`<br>• `workers‑sg`<br>• `unity-catalog-access-connector` <br><small>(Premium only)</small> | Auto-provisioned by Databricks when deploying a Premium workspace                         | • All resources are free except DBFS (`dbstorage`, usually < 5 GB).<br>• Access Connector is free               |
| **NetworkWatcherRG** (autogenerated)   | • `NetworkWatcher_<region>`                                                                                                     | Created by Azure when a VNet is created in a region                                       | Free                                                                                                            |

---

> Use `deploy_premium.bicep` **only when** Unity Catalog or dashboard features are required. The standard deployment avoids those capabilities for lower cost and simplicity.


## Deploy Instructions

Run these from your local terminal in the project root. Make sure you're logged into the correct Azure subscription.

**1. Load environment variables from .env**

```bash
source infra/.env
```

---

**2. Create the Azure resource group**

```bash
az group create --name "$RG" --location eastus
```

---

**3.  Deploy infrastructure with Bicep (Databricks + ADLS)**

```bash
# Option A – Standard (default, cheaper)
az deployment group create \
  --resource-group "$RG" \
  --template-file infra/bicep/deploy_standard.bicep \
  --name "$DEPLOY"
```

```bash
# Option B – Premium (if needed for dashboards or Unity Catalog)
az deployment group create \
  --resource-group "$RG" \
  --template-file infra/bicep/deploy_premium.bicep \
  --name "$DEPLOY"
```

---

**4. Generate a Databricks Personal Access Token (PAT)**

(Databricks UI → Settings → Developer → Generate New Token)

---

**5. Add your PAT to.env as `DATABRICKS_PAT=...`**

---

**6. Run gen_sas.sh to auto-generate and store the ADLS SAS token in Databricks**

```bash
infra/deploy/gen_sas.sh
```

---

**7. Build and push the kflow wheel to the workspace**

This will build the wheel using pyproject.toml and upload it to /Workspace/Shared/libs/ in the Databricks Workspace.

```bash
infra/deploy/build_push_kflow.sh
```

---

**8. Attach the wheel to your Databricks cluster**

(Compute → Cluster → Libraries → Install → /Shared/libs/kflow-0.1.0-py3-none-any.whl)

---

**9. Tear down all provisioned resources safely**

```bash
./infra/deploy/teardown.sh
```

The teardown script script will:

- Delete the Databricks workspace (which deletes the managed RG too)
- Delete the main resource group (kardia-rg-dev)
- Print a confirmation message
- Resources will disappear over the next 2–5 minutes.

---

### Dry-Run Deployment

To preview what the deployment will do without actually creating resources:

```bash
az deployment group what-if \
  --resource-group "$RG" \
  --template-file infra/bicep/deploy.bicep \
  --name "$DEPLOY"
```