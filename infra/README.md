# KardiaFlow Infrastructure Deployment Guide

This folder contains the infrastructure-as-code (IaC) scripts for deploying and tearing down the KardiaFlow development environment in Azure using Bicep and the Azure CLI.

---

## What It Deploys

KardiaFlow is designed for use with a **Databricks Premium-tier workspace**, enabling support for dashboards and future extensibility.

> ðŸ’¡ **Note:**  
> KardiaFlow does **not** use Unity Catalog or Delta Live Tables (DLT).  
> It relies on Delta Lake, Auto Loader, and Databricks Jobs for portability and low cost.

---

### Resource Summary

| Resource Group                     | Resources Created                                                                                                                | Notes                                                                                     | Cost Guidance                                                                                                   |
|-----------------------------------|----------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|
| **kardiaâ€‘rgâ€‘dev**                 | â€¢ `kardiaâ€‘dbx` (Databricks workspace)<br>â€¢ `kardiaadlsdemo` (ADLS Gen2 account, `raw` container)                                 | Created via Bicep                                                                         | â€¢ Workspace control-plane is free while clusters are off<br>â€¢ Storage billed by usage (LRS hot tier) |
| **kardiaâ€‘dbxâ€‘managed** (autogenerated) | â€¢ `dbmanagedidentity`<br>â€¢ `dbstorageâ€¦`<br>â€¢ `workersâ€‘vnet`<br>â€¢ `workersâ€‘sg`<br>â€¢ `unity-catalog-access-connector`             | Provisioned automatically by Databricks                                                  | â€¢ All resources are free except `dbstorage` (minimal DBFS usage)                                               |
| **NetworkWatcherRG** (autogenerated)   | â€¢ `NetworkWatcher_<region>`                                                                                                     | Created by Azure when a VNet is provisioned                                              | Free                                                                                                            |

---

## Deploy Instructions

Run these from your local terminal in the project root. Make sure you're logged into the correct Azure subscription.

**1. Load environment variables from .env**

```bash
source infra/.env
```

---

**2. Create the Azure resource group**

```bash
az group create --name "$RG" --location eastus
```

---

**3. Deploy infrastructure with Bicep (Databricks + ADLS)**

```bash
# Option B â€“ Premium (if needed for dashboards or Unity Catalog)
az deployment group create \
  --resource-group "$RG" \
  --template-file infra/bicep/deploy_premium.bicep \
  --name "$DEPLOY"
```

---

**4. Generate a Databricks Personal Access Token (PAT)**

(Databricks UI â†’ Settings â†’ Developer â†’ Generate New Token)

---

**5. Add your PAT to .env as `DATABRICKS_PAT=your_token_here`**

---

**6. Configure the Databricks CLI profile**

```bash
url=$(az databricks workspace show -g "$RG" -n "$WORKSPACE" --query "workspaceUrl" -o tsv)
databricks configure --profile "$PROFILE" --host "https://$url" --token <<< "${DATABRICKS_PAT}"$'\n'
```

If the CLI reports missing credentials, re-run `source infra/.env` to ensure environment variables are available.

---

**7. Run gen_sas.sh to auto-generate and store the ADLS SAS token in Databricks**

```bash
infra/deploy/gen_sas.sh
```

---

**8. Build and upload the wheel to DBFS**

This installs the build tool, creates the .whl package from pyproject.toml, and uploads it to DBFS.

```bash
python -m pip install --upgrade build
python -m build

# pick the newest wheel
WHL=$(ls dist/kflow-*-py3-none-any.whl | tail -n 1)

# copy the versioned wheel to DBFS
databricks fs mkdirs dbfs:/Shared/libs
databricks fs cp "$WHL" dbfs:/Shared/libs/ --overwrite
```

> ðŸ’¡ **Why DBFS?**  
> Itâ€™s the simplest way to make local Python packages available to notebooks using `%pip install --no-index --find-links=...`.  
> Compatible with DBR 13.3 LTS and avoids issues with workspace paths or job-level libraries.

---

**9. Create or reset the batch job**

```bash
# To create from scratch
databricks jobs create --json '@pipelines/jobs/kardiaflow_full_run_batch.json' --profile kardia
```

```bash
# To reset with a new wheel or config
databricks jobs reset --json @pipelines/jobs/reset_kardiaflow_full_run_batch.json
```

---

**9. Tear down all provisioned resources**

```bash
./infra/deploy/teardown.sh
```

The teardown script script will:

- Deletes the Databricks workspace (automatically removes the managed RG)
- Deletes the main resource group (kardia-rg-dev)
- Print a confirmation message
- Resources disappear within 2â€“5 minutes.

---

### When to Build a New Wheel

If you've made changes to the `kflow` package â€” such as editing any `.py` files inside the `kflow/` directory â€” follow these steps to deploy your updates:

1. Update the version number in `pyproject.toml`

`version = "0.2.6"` â†’ `version = "0.2.7"`


2. Clean old build artifacts (recommended for a fresh build):

```bash
rm -rf dist/*
```

3. Rebuild and re-upload the new wheel:

```bash
python -m build
WHL=$(ls dist/kflow-*-py3-none-any.whl | tail -n 1)
databricks fs cp "$WHL" dbfs:/Shared/libs/ --overwrite
```

This ensures all notebooks and job runs use the latest version and prevents caching issues with `%pip`.

---

### Dry-Run Deployment

To preview what the deployment will do without actually creating resources:

```bash
az deployment group what-if \
  --resource-group "$RG" \
  --template-file infra/bicep/deploy.bicep \
  --name "$DEPLOY"
```