{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04ee6be9-aa8a-4f71-9c4c-fe806381b81d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 02_silver_patients_transform.py\n",
    "# SOURCE: Ingest patient records from Bronze with PHI and promote them to Silver.\n",
    "# OUTPUT: `kardia_silver.silver_patients`, updated incrementally.\n",
    "# Read Change Data Feed from Bronze, mask PHI columns, derive `BIRTH_YEAR` from `BIRTHDATE`.\n",
    "# TRIGGER: Incremental batch job (since patient records arrive infrequently).\n",
    "\n",
    "from pyspark.sql import functions as F, Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Table paths\n",
    "SILVER_DB       = \"kardia_silver\"\n",
    "BRONZE_TABLE    = \"kardia_bronze.bronze_patients\"\n",
    "SILVER_TABLE    = f\"{SILVER_DB}.silver_patients\"\n",
    "CHECKPOINT_PATH = \"dbfs:/kardia/_checkpoints/silver_patients\"\n",
    "CHANGE_TYPES    = [\"insert\", \"update_postimage\"]\n",
    "\n",
    "PHI_COLUMNS = [\n",
    "    \"DEATHDATE\",\n",
    "    \"FIRST\",\n",
    "    \"LAST\",\n",
    "    \"MAIDEN\",\n",
    "    \"SSN\",\n",
    "    \"DRIVERS\",\n",
    "    \"PASSPORT\",\n",
    "    \"BIRTHPLACE\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ceac8f3-eb38-484a-9c2b-fea93828f68c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Ensure the Silver DB and Silver Patients table exist before writing.\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {SILVER_DB}\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {SILVER_TABLE} (\n",
    "      ID           STRING,\n",
    "      BIRTH_YEAR   INT,\n",
    "      DEATHDATE    STRING,\n",
    "      FIRST        STRING,\n",
    "      LAST         STRING,\n",
    "      MAIDEN       STRING,\n",
    "      SSN          STRING,\n",
    "      DRIVERS      STRING,\n",
    "      PASSPORT     STRING,\n",
    "      BIRTHPLACE   STRING,\n",
    "      MARITAL      STRING,\n",
    "      RACE         STRING,\n",
    "      ETHNICITY    STRING,\n",
    "      GENDER       STRING\n",
    "    ) USING DELTA\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "593477bb-2fe1-418c-a7c3-1fa3bd4ca689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Define helper function and upsert logic\n",
    "#    Since row_number() requires _commit_version, we must wait before projecting down to\n",
    "#    the final schema until foreachBatch and transform only after duplicates are removed.\n",
    "#    This is why we don't apply transformations inline (as we do in silver_encounters_transform).\n",
    "\n",
    "# Mask PHI columns and derive BIRTH_YEAR.\n",
    "def _clean_patient_data(patients_df):\n",
    "    patients_df = patients_df.withColumn(\"BIRTH_YEAR\", F.year(\"BIRTHDATE\"))\n",
    "\n",
    "    for phi_column in PHI_COLUMNS:\n",
    "        patients_df = patients_df.withColumn(phi_column, F.lit(None).cast(\"string\"))\n",
    "\n",
    "    return patients_df.select(\n",
    "        \"ID\",\n",
    "        \"BIRTH_YEAR\",\n",
    "        *PHI_COLUMNS,\n",
    "        \"MARITAL\",\n",
    "        \"RACE\",\n",
    "        \"ETHNICITY\",\n",
    "        \"GENDER\"\n",
    "    )\n",
    "\n",
    "# Merge the latest version of each patient record into the Silver Patients table.\n",
    "# `batch_df` is a static DF containing the latest new and updated rows from Bronze CDF.\n",
    "# Use the `_commit_version` column from CDF to identify the most recent change per ID.\n",
    "def upsert_to_silver(batch_df, _):\n",
    "    latest_patient_ids = (\n",
    "        batch_df.withColumn(\n",
    "                     \"row_num\",\n",
    "                     F.row_number().over(\n",
    "                         Window.partitionBy(\"ID\").orderBy(F.col(\"_commit_version\").desc())\n",
    "                     )\n",
    "                )\n",
    "                .filter(\"row_num = 1\")\n",
    "                .drop(\"row_num\")\n",
    "    )\n",
    "\n",
    "    silver_ready_df = _clean_patient_data(latest_patient_ids)\n",
    "\n",
    "    (DeltaTable.forName(spark, SILVER_TABLE)\n",
    "               .alias(\"target\")\n",
    "               .merge(silver_ready_df.alias(\"source\"), \"target.ID = source.ID\")\n",
    "               .whenMatchedUpdateAll()\n",
    "               .whenNotMatchedInsertAll()\n",
    "               .execute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fada235f-9746-441d-ae9f-14b551702766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Build source stream.\n",
    "\n",
    "# 3a. Read new inserts and updates from the Bronze Patients table.\n",
    "bronze_cdf = (\n",
    "    spark.readStream\n",
    "         .format(\"delta\")\n",
    "         .option(\"readChangeData\", \"true\")\n",
    "         .table(BRONZE_TABLE)\n",
    "         .filter(F.col(\"_change_type\").isin(*CHANGE_TYPES))\n",
    ")\n",
    "\n",
    "# 3b. Write incremental batch output.\n",
    "#     Ideal for infrequent updates like patient data.\n",
    "#     Checkpointing guarantees exactly-once processing across reruns.\n",
    "query = (bronze_cdf.writeStream\n",
    "                   .foreachBatch(upsert_to_silver)\n",
    "                   .option(\"checkpointLocation\", CHECKPOINT_PATH)\n",
    "                   .trigger(availableNow=True)\n",
    "                   .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f86f61b1-4569-4b5f-94a2-d617c8cc6e47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Print final row count and checkpoint location\n",
    "print(f\"Silver patients row count: {spark.table(SILVER_TABLE).count()}\")\n",
    "print(f\"Checkpoint: {CHECKPOINT_PATH}\")\n",
    "\n",
    "# NOTE: A Delta Lake merge is not a join in the relational sense.\n",
    "# It is a mutation command where the source table drives the operation.\n",
    "#\n",
    "# Delta CDF emits all changes since the last checkpoint.\n",
    "# This may include multiple versions of the same ID across different commits.\n",
    "# We deduplicate using row_number() to keep only the latest version per patient ID."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7755667534954233,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver_patients_transform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
