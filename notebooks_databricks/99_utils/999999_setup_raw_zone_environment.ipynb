{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33f4b7bc-5267-48b9-8952-4ab9bb91a08d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 99_utils/99_setup_kardia_autoloader_env.ipynb\n",
    "# One-time setup for a new Databricks workspace.\n",
    "# Creates raw input folders and seeds test CSVs.\n",
    "\n",
    "SEED_PATIENTS   = True\n",
    "SEED_ENCOUNTERS = True\n",
    "REPO_BASE       = \"/Workspace/Users/matthew.databrickslab2@outlook.com/kardiaflow/data/raw\"\n",
    "\n",
    "RAW_PATHS = {\n",
    "    \"patients\"  : \"dbfs:/kardia/raw/patients/\",\n",
    "    \"encounters\": \"dbfs:/kardia/raw/encounters/\"\n",
    "}\n",
    "\n",
    "SEED_FILES = {\n",
    "    \"patients\"  : f\"file:{REPO_BASE}/ehr/patients_10.csv\",\n",
    "    \"encounters\": f\"file:{REPO_BASE}/ehr/encounters_10.csv\"\n",
    "}\n",
    "\n",
    "# Create folders if missing\n",
    "for path in [\n",
    "    *RAW_PATHS.values(),\n",
    "    \"dbfs:/kardia/_schemas/\",\n",
    "    \"dbfs:/kardia/_checkpoints/\"\n",
    "]:\n",
    "    dbutils.fs.mkdirs(path)\n",
    "\n",
    "def safe_copy(src, dst):\n",
    "    try:\n",
    "        dbutils.fs.cp(src, dst, recurse=True)\n",
    "        print(f\"Copied {src} â†’ {dst}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped {src}: {e}\")\n",
    "\n",
    "if SEED_PATIENTS:\n",
    "    safe_copy(SEED_FILES[\"patients\"],   RAW_PATHS[\"patients\"])\n",
    "if SEED_ENCOUNTERS:\n",
    "    safe_copy(SEED_FILES[\"encounters\"], RAW_PATHS[\"encounters\"])\n",
    "\n",
    "for name, path in RAW_PATHS.items():\n",
    "    count = len(dbutils.fs.ls(path))\n",
    "    print(f\"{name.capitalize()} files: {count}\")\n",
    "\n",
    "print(\"Environment bootstrap complete\")\n",
    "\n",
    "# (DEMO ONLY) Minimize shuffle overhead for small test datasets\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "99_setup_kardia_autoloader_env",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
