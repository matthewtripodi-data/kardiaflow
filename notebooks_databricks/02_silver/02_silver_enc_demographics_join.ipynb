{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 03_silver_enc_demographics_join.ipynb\n",
    "# SOURCE:  Joins streaming Silver encounters with static Silver patients for enrichment.\n",
    "# OUTPUT:  `kardia_silver.silver_encounters_demographics`, written as an incremental batch.\n",
    "# PATTERN: Stream–static left join (keeps all encounters even when the patient is missing).\n",
    "# TRIGGER: Incremental batch; append to Delta table with fixed schema.\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "SILVER_ENCOUNTERS_DEMOGRAPHICS = \"kardia_silver.silver_encounters_demographics\"\n",
    "CHECKPOINT_PATH                = \"dbfs:/kardia/_checkpoints/silver_encounters_demographics\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Read existing Patients and Encounters Delta tables.\n",
    "\n",
    "# Spark treats `enc_stream` as unbounded, incremental input. (readStream)\n",
    "enc_stream = spark.readStream.table(\"kardia_silver.silver_encounters\")\n",
    "\n",
    "# Spark treats `patients_df` as a bounded snapshot. (spark.table)\n",
    "patients_df = spark.table(\"kardia_silver.silver_patients\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. Perform a left-join to retain all encounter records, including those with no matching patient.\n",
    "#    In the Gold layer, these unmatched rows are tracked for QA purposes.\n",
    "joined = (\n",
    "    enc_stream.alias(\"e\")\n",
    "              .join(\n",
    "                  patients_df.alias(\"p\"),\n",
    "                  F.col(\"e.PatientID\") == F.col(\"p.ID\"),\n",
    "                  \"left\"\n",
    "               )\n",
    "              .select(\n",
    "                  \"e.EncounterID\",\n",
    "                  \"e.PatientID\",\n",
    "                  \"e.START_TS\",\n",
    "                  \"e.START_DATE\",\n",
    "                  \"e.CODE\",\n",
    "                  \"e.DESCRIPTION\",\n",
    "                  \"e.REASONCODE\",\n",
    "                  \"e.REASONDESCRIPTION\",\n",
    "                  \"p.GENDER\",\n",
    "                  \"p.BIRTH_YEAR\"\n",
    "              )\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Write incremental batch output.\n",
    "\n",
    "#    Execution:\n",
    "#    1. Each micro-batch of new encounter rows is joined to the static patients_df snapshot.\n",
    "#    2. Spark automatically broadcasts patients_df if it is below 10 MB (autoBroadcastJoinThreshold default).\n",
    "#    3. Joined rows are appended to silver_encounters_demographics.\n",
    "action = (\n",
    "        joined.writeStream\n",
    "              .outputMode(\"append\")\n",
    "              .option(\"checkpointLocation\", CHECKPOINT_PATH)\n",
    "              .trigger(availableNow=True)\n",
    "              .table(SILVER_ENCOUNTERS_DEMOGRAPHICS)\n",
    "              .awaitTermination()\n",
    ")\n",
    "\n",
    "print(\"silver_encounters_demographics refreshed (stream‑static join)\")\n",
    "\n",
    "# NOTE:\n",
    "# - `availableNow=true` tells Spark to process all available data in micro-batches, then stop.\n",
    "# - The Encounters side refreshes every micro-batch; the Patients side is static snapshot from job start."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver_encounters_demographics_join",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
