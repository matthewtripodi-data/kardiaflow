{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50255fa6-9690-4d9e-8ffe-c2b47e0aba97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 02_silver_transform_patients_cdf_mask_pii.ipynb\n",
    "# -------------------------------------------------------\n",
    "# Ingests bronze_patients -> silver_patients\n",
    "# First run: take full snapshot -> initial Silver write\n",
    "# Later runs: read only new CDF rows and MERGE\n",
    "# SCD-1 (last-write-wins), masks PHI columns\n",
    "# -------------------------------------------------------\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from delta.tables import DeltaTable\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# Path config\n",
    "BRONZE_PATH = \"dbfs:/kardia/bronze/bronze_patients\"\n",
    "SILVER_PATH = \"dbfs:/kardia/silver/silver_patients\"\n",
    "STATE_PATH  = \"dbfs:/kardia/_state/bronze_patients_lastver.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51b6eca4-ae1d-4e59-820c-650738f54a15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_state(path, default_val):\n",
    "    try:\n",
    "        dbutils.fs.ls(path)\n",
    "        return int(dbutils.fs.head(path, 256))\n",
    "    except Exception:\n",
    "        if DeltaTable.isDeltaTable(spark, SILVER_PATH):\n",
    "            return (spark.sql(f\"DESCRIBE HISTORY delta.`{SILVER_PATH}`\")\n",
    "                      .selectExpr(\"max(version) AS v\").first().v)\n",
    "        return default_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dff1437b-93b1-4dc1-a4ef-f2f14b2ff23d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"silver_patients_incremental\")\n",
    "      .config(\"spark.sql.shuffle.partitions\", \"1\")  # dev-friendly; tune in prod\n",
    "      .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "      .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "      .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8969f49-cd8f-4a46-ab14-79d3c600923b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ── 1. Bronze metadata & CDF status ─────────────────────\n",
    "hist_df    = spark.sql(f\"DESCRIBE HISTORY delta.`{BRONZE_PATH}`\")\n",
    "latest_ver = hist_df.selectExpr(\"max(version) AS v\").first().v\n",
    "\n",
    "# 1A. Ensure CDF is ON (enable if missing)\n",
    "detail_row = spark.sql(f\"DESCRIBE DETAIL delta.`{BRONZE_PATH}`\").first()\n",
    "prop_map   = {k.lower(): v.lower() for k, v in detail_row[\"properties\"].items()}\n",
    "cdf_on_tbl = prop_map.get(\"delta.enablechangedatafeed\", \"false\") == \"true\"\n",
    "\n",
    "if not cdf_on_tbl:\n",
    "    print(\"⚙ Enabling Change Data Feed on Bronze …\")\n",
    "    spark.sql(f\"\"\"\n",
    "      ALTER TABLE delta.`{BRONZE_PATH}`\n",
    "      SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "    \"\"\")\n",
    "    # refresh metadata\n",
    "    hist_df    = spark.sql(f\"DESCRIBE HISTORY delta.`{BRONZE_PATH}`\")\n",
    "    latest_ver = hist_df.selectExpr(\"max(version) AS v\").first().v\n",
    "\n",
    "# 1B. Locate the commit that turned CDF on (robust JSON parse)\n",
    "def find_cdf_meta_version(hist_df):\n",
    "    for r in (hist_df\n",
    "              .filter(\"operation = 'SET TBLPROPERTIES'\")\n",
    "              .select(\"version\", \"operationParameters\")\n",
    "              .orderBy(\"version\")            # oldest first\n",
    "              .collect()):\n",
    "        props_json = r.operationParameters.get(\"properties\")\n",
    "        if props_json:\n",
    "            try:\n",
    "                props_dict = json.loads(props_json)\n",
    "                if props_dict.get(\"delta.enableChangeDataFeed\", \"\").lower() == \"true\":\n",
    "                    return r.version\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "    return 0   # fall back: assume enabled at CREATE TABLE (v0)\n",
    "\n",
    "first_cdf_meta_ver = find_cdf_meta_version(hist_df)\n",
    "first_cdf_data_ver = first_cdf_meta_ver + 1\n",
    "\n",
    "# If we just enabled CDF and no new WRITE has happened yet, exit early\n",
    "if latest_ver <= first_cdf_meta_ver:\n",
    "    print(\"No data commits since CDF was enabled; skipping incremental step.\")\n",
    "    dbutils.fs.put(STATE_PATH, str(latest_ver), overwrite=True)\n",
    "    dbutils.notebook.exit(\"cdf_enabled_waiting_for_next_commit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbb5d5aa-4d89-4cf1-b389-bd00127cb891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd8f6c3a-e268-4827-a489-ef760a156602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Decide run mode: bootstrap vs incremental ────────────────────────\n",
    "silver_exists = DeltaTable.isDeltaTable(spark, SILVER_PATH)\n",
    "last_processed_ver = 0\n",
    "try:\n",
    "    last_processed_ver = int(dbutils.fs.head(STATE_PATH, 256))\n",
    "except Exception:\n",
    "    last_processed_ver = first_cdf_meta_ver - 1\n",
    "\n",
    "if not silver_exists:\n",
    "    print(\"First run – bootstrapping Silver from full Bronze snapshot\")\n",
    "    \n",
    "    # Read entire Bronze, drop duplicates, apply transformations\n",
    "    base_df = (\n",
    "        spark.read.format(\"delta\").load(BRONZE_PATH)\n",
    "            .dropDuplicates([\"ID\"])  # last-write-wins semantics\n",
    "            .select(\n",
    "                \"ID\",\n",
    "                F.year(\"BIRTHDATE\").cast(\"int\").alias(\"BIRTH_YEAR\"),\n",
    "                F.lit(None).cast(\"string\").alias(\"DEATHDATE\"),\n",
    "                F.lit(None).cast(\"string\").alias(\"FIRST\"),\n",
    "                F.lit(None).cast(\"string\").alias(\"LAST\"),\n",
    "                F.lit(None).cast(\"string\").alias(\"MAIDEN\"),\n",
    "                F.lit(None).cast(\"string\").alias(\"SSN\"),\n",
    "                F.lit(None).cast(\"string\").alias(\"DRIVERS\"),\n",
    "                F.lit(None).cast(\"string\").alias(\"PASSPORT\"),\n",
    "                F.lit(None).cast(\"string\").alias(\"BIRTHPLACE\"),\n",
    "                \"MARITAL\", \"RACE\", \"ETHNICITY\", \"GENDER\"\n",
    "            )\n",
    "    )\n",
    "    base_df.write.format(\"delta\").mode(\"overwrite\").save(SILVER_PATH)\n",
    "    dbutils.fs.put(STATE_PATH, str(latest_ver), overwrite=True)\n",
    "    dbutils.notebook.exit(f\"bootstrap_complete_v{latest_ver}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58418edb-ad6c-4331-a4ea-66493377d50f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Incremental run via CDF ────────────────────────────────────────\n",
    "start_ver = max(last_processed_ver + 1, first_cdf_data_ver)\n",
    "if start_ver > latest_ver:\n",
    "    print(f\"No new Bronze commits (latest={latest_ver}, last_processed={last_processed_ver}).\")\n",
    "    dbutils.notebook.exit(\"no_new_commits\")\n",
    "\n",
    "print(f\"Reading Bronze change data feed from version {start_ver} to {latest_ver}\")\n",
    "cdf_df = (\n",
    "    spark.read\n",
    "         .format(\"delta\")\n",
    "         .option(\"readChangeFeed\", \"true\")\n",
    "         .option(\"startingVersion\", start_ver)\n",
    "         .load(BRONZE_PATH)\n",
    "         .filter(F.col(\"_change_type\").isin(\"insert\", \"update_postimage\"))\n",
    "         .dropDuplicates([\"ID\"])\n",
    "         .select(\n",
    "             \"ID\",\n",
    "             F.year(\"BIRTHDATE\").cast(\"int\").alias(\"BIRTH_YEAR\"),\n",
    "             F.lit(None).cast(\"string\").alias(\"DEATHDATE\"),\n",
    "             F.lit(None).cast(\"string\").alias(\"FIRST\"),\n",
    "             F.lit(None).cast(\"string\").alias(\"LAST\"),\n",
    "             F.lit(None).cast(\"string\").alias(\"MAIDEN\"),\n",
    "             F.lit(None).cast(\"string\").alias(\"SSN\"),\n",
    "             F.lit(None).cast(\"string\").alias(\"DRIVERS\"),\n",
    "             F.lit(None).cast(\"string\").alias(\"PASSPORT\"),\n",
    "             F.lit(None).cast(\"string\").alias(\"BIRTHPLACE\"),\n",
    "             \"MARITAL\", \"RACE\", \"ETHNICITY\", \"GENDER\"\n",
    "         )\n",
    ")\n",
    "\n",
    "if cdf_df.rdd.isEmpty():\n",
    "    print(\"No row-level changes in this commit range.\")\n",
    "    dbutils.fs.put(STATE_PATH, str(latest_ver), overwrite=True)\n",
    "    dbutils.notebook.exit(\"empty_cdf_batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4540dcef-c87a-492c-9f5c-abd0e8145e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Merge changes into Silver (null-safe for all fields)\n",
    "tgt = DeltaTable.forPath(spark, SILVER_PATH)\n",
    "\n",
    "# Perform update with null-protection (SCD-1 style, last-write-wins)\n",
    "(\n",
    "    tgt.alias(\"t\")\n",
    "        .merge(cdf_df.alias(\"s\"), \"t.ID = s.ID\")\n",
    "        .whenMatchedUpdate(set={\n",
    "            \"ID\":           \"coalesce(s.ID, t.ID)\",\n",
    "            \"BIRTH_YEAR\":   \"coalesce(s.BIRTH_YEAR, t.BIRTH_YEAR)\",\n",
    "            \"DEATHDATE\":    \"coalesce(s.DEATHDATE, t.DEATHDATE)\",\n",
    "            \"FIRST\":        \"coalesce(s.FIRST, t.FIRST)\",\n",
    "            \"LAST\":         \"coalesce(s.LAST, t.LAST)\",\n",
    "            \"MAIDEN\":       \"coalesce(s.MAIDEN, t.MAIDEN)\",\n",
    "            \"SSN\":          \"coalesce(s.SSN, t.SSN)\",\n",
    "            \"DRIVERS\":      \"coalesce(s.DRIVERS, t.DRIVERS)\",\n",
    "            \"PASSPORT\":     \"coalesce(s.PASSPORT, t.PASSPORT)\",\n",
    "            \"BIRTHPLACE\":   \"coalesce(s.BIRTHPLACE, t.BIRTHPLACE)\",\n",
    "            \"MARITAL\":      \"coalesce(s.MARITAL, t.MARITAL)\",\n",
    "            \"RACE\":         \"coalesce(s.RACE, t.RACE)\",\n",
    "            \"ETHNICITY\":    \"coalesce(s.ETHNICITY, t.ETHNICITY)\",\n",
    "            \"GENDER\":       \"coalesce(s.GENDER, t.GENDER)\"\n",
    "        })\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f553d0cd-4d01-4872-9c31-75381ac79ab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Persist state and final checks ─────────────────────────────────\n",
    "dbutils.fs.put(STATE_PATH, str(latest_ver), overwrite=True)\n",
    "print(f\"✔ Silver upsert complete (processed through version {latest_ver})\")\n",
    "cnt = spark.read.format(\"delta\").load(SILVER_PATH).count()\n",
    "print(f\"Silver row count: {cnt}\")\n",
    "\n",
    "# Preview final rows\n",
    "display(spark.read.format(\"delta\").load(SILVER_PATH).limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4250402b-41b0-46f0-9a96-9c2655b6e28e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Silver complete  \n",
    "## Proceed to gold view creation `03_gold/create_gold_views`"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver_transform_patients_cdf_mask_pii",
   "widgets": {
    "env": {
     "currentValue": "prod",
     "nuid": "ec8af182-718d-4553-9f2f-08ad37e9603b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "prod",
      "label": null,
      "name": "env",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "prod",
      "label": null,
      "name": "env",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
