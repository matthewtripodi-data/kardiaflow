{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50255fa6-9690-4d9e-8ffe-c2b47e0aba97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 02_silver_patients_transform.ipynb\n",
    "# Promote Bronze patient data to Silver, masking PHI columns.\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Paths and table names\n",
    "SILVER_DB = \"kardia_silver\"\n",
    "SILVER_PATIENTS_TABLE = f\"{SILVER_DB}.silver_patients\"\n",
    "BOOKMARK_FILE = \"dbfs:/kardia/_state/bronze_to_silver_patients.txt\"\n",
    "BRONZE_PATH = \"dbfs:/kardia/bronze/bronze_patients\"\n",
    "\n",
    "# Minimize shuffle overhead for small test datasets\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2cd8345-76b0-48ed-995c-a5350a1e2d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE DATABASE IF NOT EXISTS kardia_silver;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS kardia_silver.silver_patients AS\n",
    "SELECT\n",
    "    ID,\n",
    "    year(BIRTHDATE) AS BIRTH_YEAR,\n",
    "    -- mask PHI columns\n",
    "    CAST(NULL AS STRING) AS DEATHDATE,\n",
    "    CAST(NULL AS STRING) AS FIRST,\n",
    "    CAST(NULL AS STRING) AS LAST,\n",
    "    CAST(NULL AS STRING) AS MAIDEN,\n",
    "    CAST(NULL AS STRING) AS SSN,\n",
    "    CAST(NULL AS STRING) AS DRIVERS,\n",
    "    CAST(NULL AS STRING) AS PASSPORT,\n",
    "    CAST(NULL AS STRING) AS BIRTHPLACE,\n",
    "    MARITAL,\n",
    "    RACE,\n",
    "    ETHNICITY,\n",
    "    GENDER\n",
    "FROM kardia_bronze.bronze_patients;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51b6eca4-ae1d-4e59-820c-650738f54a15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the last processed Bronze version (defaults to -1 on first run)\n",
    "try:\n",
    "    last_processed = int(dbutils.fs.head(BOOKMARK_FILE, 1024))\n",
    "except Exception:\n",
    "    last_processed = -1\n",
    "\n",
    "latest_bronze = (\n",
    "    spark.sql(f\"DESCRIBE HISTORY delta.`{BRONZE_PATH}` LIMIT 1\").first().version\n",
    ")\n",
    "\n",
    "if last_processed < latest_bronze:\n",
    "\n",
    "    # Pull only the changed rows from Change Data Feed\n",
    "    bronze_updates = (\n",
    "        spark.read\n",
    "            .format(\"delta\")\n",
    "            .option(\"readChangeFeed\", \"true\")\n",
    "            .option(\"startingVersion\", last_processed + 1)\n",
    "            .load(BRONZE_PATH)\n",
    "            .filter(F.col(\"_change_type\").isin(\"insert\", \"update_postimage\"))\n",
    "    )\n",
    "\n",
    "    # Use a window to retain only the latest change per ID when multiple updates exist\n",
    "    latest_commit_per_id = Window.partitionBy(\"ID\").orderBy(F.col(\"_commit_version\").desc())\n",
    "\n",
    "    bronze_updates_latest = (\n",
    "        bronze_updates\n",
    "            .withColumn(\"rn\", F.row_number().over(latest_commit_per_id))\n",
    "            .filter(F.col(\"rn\") == 1)\n",
    "            .drop(\"rn\")\n",
    "    )\n",
    "\n",
    "    # Build the Silver rows: derive BIRTH_YEAR and mask PHI\n",
    "    silver_rows = bronze_updates_latest.withColumn(\"BIRTH_YEAR\", F.year(\"BIRTHDATE\"))\n",
    "\n",
    "    PHI_COLUMNS = [\n",
    "        \"DEATHDATE\", \"FIRST\", \"LAST\", \"MAIDEN\", \"SSN\",\n",
    "        \"DRIVERS\", \"PASSPORT\", \"BIRTHPLACE\"\n",
    "    ]\n",
    "\n",
    "    for c in PHI_COLUMNS:\n",
    "        silver_rows = silver_rows.withColumn(c, F.lit(None).cast(\"string\"))\n",
    "\n",
    "    silver_rows = silver_rows.select(\n",
    "        \"ID\", \"BIRTH_YEAR\", *PHI_COLUMNS, \"MARITAL\", \"RACE\", \"ETHNICITY\", \"GENDER\"\n",
    "    )\n",
    "\n",
    "    # Enable auto-merge so a future nullable column won't break the pipeline\n",
    "    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "    # Run the SCD-1 merge\n",
    "    (\n",
    "        DeltaTable.forName(spark, SILVER_PATIENTS_TABLE)\n",
    "                .alias(\"t\")\n",
    "                .merge(silver_rows.alias(\"s\"), \"t.ID = s.ID\")\n",
    "                .whenMatchedUpdateAll()\n",
    "                .whenNotMatchedInsertAll()\n",
    "                .execute()\n",
    "    )\n",
    "\n",
    "    print(\"Silver patients table updated.\")\n",
    "else:\n",
    "    print(\"No new commits. Silver patients is already current.\")\n",
    "\n",
    "# Write the updated latest_bronze version back to the bookmark file,\n",
    "# ensuring on the next run only new changes will be pulled.\n",
    "dbutils.fs.put(BOOKMARK_FILE, str(latest_bronze), overwrite=True)\n",
    "print(f\"Saved latest processed version: {latest_bronze}\")\n",
    "\n",
    "# Sanity check\n",
    "print(f\"Row count: {spark.table(SILVER_PATIENTS_TABLE).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f553d0cd-4d01-4872-9c31-75381ac79ab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Refresh and show summary\n",
    "spark.sql(f\"REFRESH TABLE {SILVER_PATIENTS_TABLE}\")\n",
    "print(f\"Row count: {spark.table(SILVER_PATIENTS_TABLE).count()}\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    SELECT version, timestamp, operation\n",
    "    FROM (DESCRIBE HISTORY {SILVER_PATIENTS_TABLE})\n",
    "    ORDER BY version DESC\n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    ").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4948985473729843,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver_patients_transform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
