{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04ee6be9-aa8a-4f71-9c4c-fe806381b81d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 02_silver_patients_transform.ipynb\n",
    "# Promote Bronze patient data to Silver, masking PHI columns.\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Table paths\n",
    "SILVER_DB = \"kardia_silver\"\n",
    "SILVER_PATIENTS_TABLE = f\"{SILVER_DB}.silver_patients\"\n",
    "BOOKMARK_FILE = \"dbfs:/kardia/_state/bronze_to_silver_patients.txt\"\n",
    "BRONZE_PATH = \"dbfs:/kardia/bronze/bronze_patients\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ceac8f3-eb38-484a-9c2b-fea93828f68c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE DATABASE IF NOT EXISTS kardia_silver;\n",
    "\n",
    "-- Define schema for silver_patients and apply tranformations, materializing\n",
    "-- the first version of the data with the required business logic built in.\n",
    "-- CTAS locks schema, data, and privacy intent into one readable statement.\n",
    "CREATE TABLE IF NOT EXISTS kardia_silver.silver_patients AS\n",
    "SELECT\n",
    "    ID,\n",
    "    year(BIRTHDATE) AS BIRTH_YEAR,\n",
    "    -- mask PHI columns\n",
    "    CAST(NULL AS STRING) AS DEATHDATE,\n",
    "    CAST(NULL AS STRING) AS FIRST,\n",
    "    CAST(NULL AS STRING) AS LAST,\n",
    "    CAST(NULL AS STRING) AS MAIDEN,\n",
    "    CAST(NULL AS STRING) AS SSN,\n",
    "    CAST(NULL AS STRING) AS DRIVERS,\n",
    "    CAST(NULL AS STRING) AS PASSPORT,\n",
    "    CAST(NULL AS STRING) AS BIRTHPLACE,\n",
    "    MARITAL,\n",
    "    RACE,\n",
    "    ETHNICITY,\n",
    "    GENDER\n",
    "FROM kardia_bronze.bronze_patients;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "593477bb-2fe1-418c-a7c3-1fa3bd4ca689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# GOAL: Incrementally merge new and updated Bronze patient records into Silver using CDF and version bookmarks.\n",
    "#       Apply PHI masking, derive BIRTH_YEAR, and ensure only latest change per ID is merged (SCD Type 1).\n",
    "\n",
    "# STEP 1: Determine if new Bronze data exists using version-based bookmarking.\n",
    "#         Read the last processed Bronze version from the bookmark file (defaults to -1 on first run)\n",
    "try:\n",
    "    last_processed = int(dbutils.fs.head(BOOKMARK_FILE, 1024))\n",
    "except Exception:\n",
    "    last_processed = -1\n",
    "\n",
    "# Read the current version of the Bronze table\n",
    "latest_bronze = (\n",
    "    spark.sql(f\"DESCRIBE HISTORY delta.`{BRONZE_PATH}` LIMIT 1\").first().version\n",
    ")\n",
    "\n",
    "# STEP 2: If there is new data to process, read the CDF from the latest version.\n",
    "if last_processed < latest_bronze:\n",
    "    # Pull only the changed rows from Change Data Feed\n",
    "    bronze_updates = (\n",
    "        spark.read\n",
    "            .format(\"delta\")\n",
    "            .option(\"readChangeFeed\", \"true\")\n",
    "            .option(\"startingVersion\", last_processed + 1)\n",
    "            .load(BRONZE_PATH)\n",
    "            .filter(F.col(\"_change_type\").isin(\"insert\", \"update_postimage\"))\n",
    "    )\n",
    "\n",
    "    # The Delta CDF may emit multiple rows for the same ID across versions.\n",
    "    # We use a window to retain only the latest change per ID when multiple updates exist.\n",
    "    latest_commit_per_id = Window.partitionBy(\"ID\").orderBy(F.col(\"_commit_version\").desc())\n",
    "\n",
    "    bronze_updates_latest = (\n",
    "        bronze_updates\n",
    "            .withColumn(\"rn\", F.row_number().over(latest_commit_per_id))\n",
    "            .filter(F.col(\"rn\") == 1)\n",
    "            .drop(\"rn\")\n",
    "    )\n",
    "\n",
    "    # Build the Silver rows: derive BIRTH_YEAR and mask PHI\n",
    "    silver_rows = bronze_updates_latest.withColumn(\"BIRTH_YEAR\", F.year(\"BIRTHDATE\"))\n",
    "\n",
    "    PHI_COLUMNS = [\n",
    "        \"DEATHDATE\", \"FIRST\", \"LAST\", \"MAIDEN\", \"SSN\",\n",
    "        \"DRIVERS\", \"PASSPORT\", \"BIRTHPLACE\"\n",
    "    ]\n",
    "\n",
    "    # Create a column of NULL values with string type to overwrite sensitive PHI fields\n",
    "    for c in PHI_COLUMNS:\n",
    "        silver_rows = silver_rows.withColumn(c, F.lit(None).cast(\"string\"))\n",
    "\n",
    "    silver_rows = silver_rows.select(\n",
    "        \"ID\", \"BIRTH_YEAR\", *PHI_COLUMNS, \"MARITAL\", \"RACE\", \"ETHNICITY\", \"GENDER\"\n",
    "    )\n",
    "\n",
    "    # Enable auto-merge so a future nullable column won't break the pipeline\n",
    "    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "    # STEP 3: Perform an upsert of the latest patient rows (silver_rows) into existing\n",
    "    #         Silver table SILVER_PATIENTS_TABLE using SCD-1 logic.\n",
    "    # - If the record ID exists, overwrite it with the latest values.\n",
    "    # - If the record does not exist, insert it as a new row.\n",
    "    (\n",
    "        DeltaTable.forName(spark, SILVER_PATIENTS_TABLE)\n",
    "                .alias(\"t\")\n",
    "                .merge(silver_rows.alias(\"s\"), \"t.ID = s.ID\")\n",
    "                .whenMatchedUpdateAll()\n",
    "                .whenNotMatchedInsertAll()\n",
    "                .execute()\n",
    "    )\n",
    "\n",
    "    print(\"Silver patients table updated.\")\n",
    "else:\n",
    "    print(\"No new commits. Silver patients is already current.\")\n",
    "\n",
    "# STEP 4: Write the updated latest_bronze version back to the bookmark file,\n",
    "#         ensuring on the next run only new changes will be pulled.\n",
    "dbutils.fs.put(BOOKMARK_FILE, str(latest_bronze), overwrite=True)\n",
    "print(f\"Saved latest processed version: {latest_bronze}\")\n",
    "\n",
    "# Sanity check\n",
    "print(f\"Row count: {spark.table(SILVER_PATIENTS_TABLE).count()}\")\n",
    "\n",
    "# NOTE\n",
    "\n",
    "# A Delta Lake merge is not a join in the relational sense.\n",
    "# It is a mutation command where the source DataFrame drives the operation.\n",
    "\n",
    "# Delta reads all CDF records from the `startingVersion` up to and including the latest commit\n",
    "# So even if multiple commits occurred since the last run, all changes will be included in the read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fada235f-9746-441d-ae9f-14b551702766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Refresh and show summary\n",
    "spark.sql(f\"REFRESH TABLE {SILVER_PATIENTS_TABLE}\")\n",
    "print(f\"Row count: {spark.table(SILVER_PATIENTS_TABLE).count()}\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    SELECT version, timestamp, operation\n",
    "    FROM (DESCRIBE HISTORY {SILVER_PATIENTS_TABLE})\n",
    "    ORDER BY version DESC\n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# NOTE: Without a REFRESH, you might see incorrect results when querying the table."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7755667534954233,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver_patients_transform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
