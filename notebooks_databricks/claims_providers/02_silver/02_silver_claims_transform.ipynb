{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f1c825f-436f-4956-8821-b42a05e210dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 02_silver_claims_current_stream.py\n",
    "# SOURCE:  kardia_bronze.bronze_claims  (Delta Change Data Feed ON)\n",
    "# OUTPUT:  kardia_silver.silver_claims_current  (SCD‑1: latest row per ClaimID)\n",
    "# PATTERN: Structured Streaming CDF + foreachBatch upsert (mirrors 02_silver_patients_transform.py style).\n",
    "# TRIGGER: Incremental batch via trigger(availableNow=True); checkpoint drives progress.\n",
    "# STATE: Streaming checkpoint replaces the prior __pipeline_metadata version‑tracking table.\n",
    "#        If checkpoint is lost, full Bronze history will replay; MERGE keeps target idempotent.\n",
    "# AUDIT: Adds _ingest_ts at Silver load time (used downstream in Gold hourly metrics).\n",
    "\n",
    "from pyspark.sql import functions as F, Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Table paths\n",
    "SILVER_DB       = \"kardia_silver\"\n",
    "BRONZE_TABLE    = \"kardia_bronze.bronze_claims\"\n",
    "SILVER_TABLE    = f\"{SILVER_DB}.silver_claims_current\"\n",
    "CHECKPOINT_PATH = \"dbfs:/kardia/_checkpoints/silver_claims\"\n",
    "CHANGE_TYPES    = [\"insert\", \"update_postimage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d07f174-e9ea-4e48-b620-05d26ffcd8f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Ensure Silver DB & target table exist.\n",
    "#    We declare the Silver schema explicitly (adds _ingest_ts) so downstream\n",
    "#    code is stable even if Bronze evolves.\n",
    "\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {SILVER_DB}\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {SILVER_TABLE} (\n",
    "      ClaimID               STRING,\n",
    "      PatientID             STRING,\n",
    "      ProviderID            STRING,\n",
    "      ClaimAmount           DOUBLE,\n",
    "      ClaimDate             STRING,\n",
    "      DiagnosisCode         STRING,\n",
    "      ProcedureCode         STRING,\n",
    "      ClaimStatus           STRING,\n",
    "      ClaimType             STRING,\n",
    "      ClaimSubmissionMethod STRING,\n",
    "      _ingest_ts            TIMESTAMP\n",
    "    ) USING DELTA\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89369afa-d960-42cf-b49e-36569b28d896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2.  Helper: prepare batch for SCD‑1 upsert.\n",
    "#     Deduplicate within the micro‑batch by ClaimID ordering on _commit_version desc.\n",
    "#     Drop CDF system columns; stamp _ingest_ts.\n",
    "\n",
    "def _prepare_claims_df(batch_df):\n",
    "    w = Window.partitionBy(\"ClaimID\").orderBy(F.col(\"_commit_version\").desc())\n",
    "    latest = (batch_df\n",
    "              .withColumn(\"row_num\", F.row_number().over(w))\n",
    "              .filter(\"row_num = 1\")\n",
    "              .drop(\"row_num\", \"_change_type\", \"_commit_version\", \"_commit_timestamp\"))\n",
    "    return latest.withColumn(\"_ingest_ts\", F.current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9546444d-3242-4497-8cc5-61f42353e373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3 ▸ foreachBatch upsert: SCD‑1 current image per ClaimID.\n",
    "\n",
    "def upsert_to_silver(batch_df, _):\n",
    "    silver_ready_df = _prepare_claims_df(batch_df)\n",
    "    (DeltaTable.forName(spark, SILVER_TABLE)\n",
    "               .alias(\"t\")\n",
    "               .merge(silver_ready_df.alias(\"s\"), \"t.ClaimID = s.ClaimID\")\n",
    "               .whenMatchedUpdateAll()\n",
    "               .whenNotMatchedInsertAll()\n",
    "               .execute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d6535a9-ee2a-4c45-b6dc-db5b3b772cda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4 ▸ Build CDF source stream from Bronze Claims.\n",
    "#     We read Change Data Feed and keep only post‑image rows.\n",
    "bronze_cdf = (\n",
    "    spark.readStream\n",
    "         .format(\"delta\")\n",
    "         .option(\"readChangeData\", \"true\")\n",
    "         .table(BRONZE_TABLE)\n",
    "         .filter(F.col(\"_change_type\").isin(*CHANGE_TYPES))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2dea42e-a53f-4a75-b99f-15a6a5ddcede",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5 ▸ Start incremental availableNow run.\n",
    "#     Checkpoint tracks source progress; reruns pick up only new commits.\n",
    "query = (\n",
    "    bronze_cdf.writeStream\n",
    "              .foreachBatch(upsert_to_silver)\n",
    "              .option(\"checkpointLocation\", CHECKPOINT_PATH)\n",
    "              .trigger(availableNow=True)\n",
    "              .start()\n",
    ")\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c270d12-3079-46e3-a7d1-d45e3fcdd649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6 ▸ Post‑run sanity checks.\n",
    "print(f\"Silver claims row count: {spark.table(SILVER_TABLE).count()}\")\n",
    "print(f\"Checkpoint: {CHECKPOINT_PATH}\")\n",
    "\n",
    "# Preview\n",
    "display(spark.table(SILVER_TABLE).limit(20))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver_claims_transform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
