{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bcd3903-19d1-4296-95d3-fa9e8d6ccbd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 02_silver_providers_transform.py\n",
    "# SOURCE : kardia_bronze.bronze_providers  (Bronze appends snapshots; _ingest_ts present; CDF available but unused here)\n",
    "# OUTPUT : kardia_silver.silver_providers_dim  (SCD‑2 w/ eff_start_ts, eff_end_ts, is_current)\n",
    "# PATTERN: Simple batch full‑snapshot compare (demo scale). Phase 1 close changed rows; Phase 2 insert new/changed.\n",
    "# NOTE   : eff_start_ts taken from Bronze _ingest_ts (fallback now()) to avoid artificial history churn.\n",
    "\n",
    "from pyspark.sql import functions as F, Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Table paths\n",
    "BRONZE_TABLE = \"kardia_bronze.bronze_providers\"\n",
    "SILVER_DB    = \"kardia_silver\"\n",
    "DIM_TABLE    = f\"{SILVER_DB}.silver_providers_dim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0596a7c-fa5b-4d8f-a25f-5806ac894047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Ensure DB + Dim table exist\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {SILVER_DB}\")\n",
    "\n",
    "# Create empty dimension table on first run\n",
    "if not spark.catalog.tableExists(DIM_TABLE):\n",
    "    empty = spark.table(BRONZE_TABLE).limit(0)\n",
    "    (empty.withColumn(\"eff_start_ts\", F.current_timestamp())\n",
    "          .withColumn(\"eff_end_ts\",   F.lit(None).cast(\"timestamp\"))\n",
    "          .withColumn(\"is_current\",   F.lit(True))\n",
    "          .write.format(\"delta\").saveAsTable(DIM_TABLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ec8588c-0ecd-4d4e-ac6a-8fe0719a6cd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Latest snapshot per ProviderID from Bronze\n",
    "bronze = spark.table(BRONZE_TABLE)\n",
    "w      = Window.partitionBy(\"ProviderID\").orderBy(F.col(\"_ingest_ts\").desc())\n",
    "\n",
    "latest_src = (bronze\n",
    "              .withColumn(\"_rn\", F.row_number().over(w))\n",
    "              .filter(\"_rn = 1\")\n",
    "              .drop(\"_rn\")\n",
    "              .withColumn(\"eff_start_ts\", F.col(\"_ingest_ts\").cast(\"timestamp\"))\n",
    "              .withColumn(\"eff_end_ts\",   F.lit(None).cast(\"timestamp\"))\n",
    "              .withColumn(\"is_current\",   F.lit(True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11a75704-6abc-451c-8d6d-5bb81f2453d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. One‑shot MERGE (close & insert)\n",
    "dim = DeltaTable.forName(spark, DIM_TABLE)\n",
    "\n",
    "change_pred = (\n",
    "    \"NOT (t.ProviderSpecialty <=> s.ProviderSpecialty) OR \"\n",
    "    \"NOT (t.ProviderLocation  <=> s.ProviderLocation)\"\n",
    ")\n",
    "\n",
    "(dim.alias(\"t\")\n",
    "    .merge(\n",
    "        latest_src.alias(\"s\"),\n",
    "        \"t.ProviderID = s.ProviderID AND t.is_current = true\"\n",
    "    )\n",
    "    # 1 ▸ Close the existing version when a change is detected\n",
    "    .whenMatchedUpdate(\n",
    "        condition=change_pred,\n",
    "        set={\n",
    "            \"eff_end_ts\": F.col(\"s.eff_start_ts\"),\n",
    "            \"is_current\": F.lit(False)\n",
    "        }\n",
    "    )\n",
    "    # 2 ▸ Insert brand‑new providers OR new versions of changed rows\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute())\n",
    "\n",
    "print(f\"SCD‑2 dimension refreshed: {DIM_TABLE}.  Row count: {spark.table(DIM_TABLE).count()}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver_providers_transform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
