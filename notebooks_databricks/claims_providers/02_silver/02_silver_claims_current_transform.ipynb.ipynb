{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f1c825f-436f-4956-8821-b42a05e210dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 02_silver_claims_current.ipynb\n",
    "# SOURCE:  kardia_bronze.bronze_claims  (Delta CDF ON)\n",
    "# OUTPUT:  kardia_silver.silver_claims_current  (latest row per ClaimID)\n",
    "# TRIGGER: Batch job; keeps progress in kardia_silver.__pipeline_metadata.\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql  import functions as F, Window\n",
    "\n",
    "# Table paths\n",
    "BRONZE_TABLE = \"kardia_bronze.bronze_claims\"\n",
    "SILVER_TABLE = \"kardia_silver.silver_claims_current\"\n",
    "META_TABLE   = \"kardia_silver.__pipeline_metadata\"\n",
    "META_KEY     = \"claims_last_version\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d07f174-e9ea-4e48-b620-05d26ffcd8f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Ensure Silver DB and meta table exist\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS kardia_silver\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {META_TABLE} (\n",
    "      key          STRING,\n",
    "      last_version LONG\n",
    "    ) USING DELTA\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89369afa-d960-42cf-b49e-36569b28d896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Determine last processed Bronze commit version\n",
    "last_version = (\n",
    "    spark.sql(\n",
    "        f\"SELECT COALESCE(MAX(last_version), -1) AS v \"\n",
    "        f\"FROM {META_TABLE} WHERE key = '{META_KEY}'\"\n",
    "    )\n",
    "    .first()\n",
    "    .v\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9546444d-3242-4497-8cc5-61f42353e373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Read new inserts/updates from Bronze via CDF\n",
    "cdf_df = (\n",
    "    spark.read.format(\"delta\")\n",
    "              .option(\"readChangeData\", \"true\")\n",
    "              .option(\"startingVersion\", last_version + 1)\n",
    "              .table(BRONZE_TABLE)\n",
    "              .filter(\"_change_type IN ('insert', 'update_postimage')\")\n",
    ")\n",
    "\n",
    "if cdf_df.limit(1).count() == 0:\n",
    "    print(\"No new changes – exiting.\")\n",
    "    dbutils.notebook.exit(\"SKIPPED\")\n",
    "\n",
    "new_version = cdf_df.agg(F.max(\"_commit_version\").alias(\"v\")).first().v\n",
    "\n",
    "claims_df = (\n",
    "    cdf_df.drop(\"_change_type\", \"_commit_version\", \"_commit_timestamp\")\n",
    "          .withColumn(\"_ingest_ts\", F.current_timestamp())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d6535a9-ee2a-4c45-b6dc-db5b3b772cda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Deduplicate → keep latest _commit_version per ClaimID\n",
    "w = Window.partitionBy(\"ClaimID\").orderBy(F.col(\"_commit_version\").desc())\n",
    "\n",
    "latest_df = (\n",
    "    cdf_df.withColumn(\"rn\", F.row_number().over(w))\n",
    "          .filter(\"rn = 1\")\n",
    "          .drop(\"rn\")\n",
    ")\n",
    "\n",
    "new_version = latest_df.agg(F.max(\"_commit_version\").alias(\"v\")).first().v\n",
    "\n",
    "claims_df = (\n",
    "    latest_df.drop(\"_change_type\", \"_commit_version\", \"_commit_timestamp\")\n",
    "             .withColumn(\"_ingest_ts\", F.current_timestamp())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2dea42e-a53f-4a75-b99f-15a6a5ddcede",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Upsert (SCD‑1) into Silver table\n",
    "if not spark.catalog.tableExists(SILVER_TABLE):\n",
    "    claims_df.limit(0).write \\\n",
    "             .format(\"delta\") \\\n",
    "             .option(\"mergeSchema\", \"true\") \\\n",
    "             .saveAsTable(SILVER_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c270d12-3079-46e3-a7d1-d45e3fcdd649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. Upsert (SCD‑1) into Silver\n",
    "(\n",
    "    DeltaTable.forName(spark, SILVER_TABLE)\n",
    "              .alias(\"t\")\n",
    "              .merge(claims_df.alias(\"s\"), \"t.ClaimID = s.ClaimID\")\n",
    "              .whenMatchedUpdateAll()\n",
    "              .whenNotMatchedInsertAll()\n",
    "              .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fca9b42-c6aa-4a52-8b8b-11b95f5371be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7. Persist latest processed version\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    MERGE INTO {META_TABLE} t\n",
    "    USING (SELECT '{META_KEY}' AS key, {new_version} AS last_version) s\n",
    "    ON t.key = s.key\n",
    "    WHEN MATCHED THEN UPDATE SET last_version = s.last_version\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(f\"Processed Bronze through version {new_version}.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver_claims_current_transform.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
