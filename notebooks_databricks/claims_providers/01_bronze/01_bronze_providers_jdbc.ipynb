{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e992afae-c826-45bd-91c6-93f063c1f90c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 01_bronze_jdbc_providers.ipynb\n",
    "# SOURCE:  Dockerâ€‘local Postgres table `providers`\n",
    "# OUTPUT:  `kardia_bronze.bronze_providers` with Change Data Feed enabled\n",
    "# PATTERN: Incremental batch; append to Delta table with fixed schema.\n",
    "\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Table paths\n",
    "BRONZE_DB         = \"kardia_bronze\"\n",
    "BRONZE_PROV_TABLE = f\"{BRONZE_DB}.bronze_providers\"\n",
    "BRONZE_PATH       = \"dbfs:/kardia/bronze/bronze_providers\"\n",
    "\n",
    "# JDBC connection (local container started by init script)\n",
    "JDBC_URL = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "PG_USER  = \"postgres\"\n",
    "PG_PW    = dbutils.secrets.get(\"kardia\", \"pg_pw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b609a2f-04b6-4330-90da-f2534b9a8eda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Ensure Bronze DB and Providers table exist.\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {BRONZE_DB}\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {BRONZE_PROV_TABLE}\n",
    "    USING DELTA\n",
    "    COMMENT 'Bronze JDBC ingest of provider reference data.'\n",
    "    LOCATION '{BRONZE_PATH}'\n",
    "    TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true')\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb56413c-b1a6-4e72-ba58-5eb5f3566a51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Read Postgres snapshot and append into Bronze Providers table.\n",
    "\n",
    "#    Snapshot read of the entire table.\n",
    "#    In production, use incremental logic (WHERE updated_at > last_ingested)\n",
    "provider_df = (spark.read\n",
    "                    .format(\"jdbc\")\n",
    "                    .option(\"url\",      JDBC_URL)\n",
    "                    .option(\"dbtable\",  \"providers\")\n",
    "                    .option(\"user\",     PG_USER)\n",
    "                    .option(\"password\", PG_PW)\n",
    "                    .load()\n",
    "                    .withColumn(\"_ingest_ts\", F.current_timestamp())\n",
    "              )\n",
    "(provider_df.write\n",
    "            .format(\"delta\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .mode(\"append\")\n",
    "            .saveAsTable(BRONZE_PROV_TABLE))\n",
    "\n",
    "print(f\"Bronze ingest complete from Postgres to {BRONZE_PROV_TABLE}\")\n",
    "\n",
    "# NOTE: Because we transform the DataFrame, we cannot chain .write directly onto .read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52d79013-e055-4cce-a16d-234fa80c73a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Quick sanity check.\n",
    "print(f\"Row count: {spark.table(BRONZE_PROV_TABLE).count()}\")\n",
    "display(spark.table(BRONZE_PROV_TABLE).limit(10))\n",
    "\n",
    "history_df = (spark.sql(f\"DESCRIBE HISTORY {BRONZE_PROV_TABLE}\")\n",
    "                   .select(\"version\",\"timestamp\",\"operation\"))\n",
    "display(history_df.limit(3))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_providers_jdbc",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
